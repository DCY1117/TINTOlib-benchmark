{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: TINTOlib in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (0.0.25)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from pandas) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jiayu\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jiayu\\anaconda3\\envs\\tinto-p\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops pandas TINTOlib scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# einops library for tensor operations\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "# Custom TINTO library imports\n",
    "from TINTOlib.tinto import TINTO\n",
    "from TINTOlib.supertml import SuperTML\n",
    "from TINTOlib.igtd import IGTD\n",
    "from TINTOlib.refined import REFINED\n",
    "from TINTOlib.barGraph import BarGraph\n",
    "from TINTOlib.distanceMatrix import DistanceMatrix\n",
    "from TINTOlib.combination import Combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression ViT+MLP\n",
    "\n",
    "## Table of Contents\n",
    "1. [Dataset](#dataset)\n",
    "2. [Load and Preprocess](#load-and-preprocess)\n",
    "3. [Model Architectures](#model-architectures)\n",
    "4. [Metrics](#metrics)\n",
    "5. [Compile and Fit](#compile-and-fit)\n",
    "6. [Experiments](#experiments)\n",
    "    - [Experiment 1: TINTO](#experiment-1-tinto)\n",
    "    - [Experiment 2: IGTD](#experiment-2-igtd)\n",
    "    - [Experiment 3: REFINED](#experiment-3-refined)\n",
    "    - [Experiment 4: Bar Graph](#experiment-4-bar-graph)\n",
    "    - [Experiment 5: Distance Matrix](#experiment-5-distance-matrix)\n",
    "    - [Experiment 6: Combination](#experiment-6-combination)\n",
    "    - [Experiment 7: SuperTML](#experiment-7-supertml)\n",
    "7. [Final Metrics and Best Model](#final-metrics-and-best-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable to store dataset name\n",
    "dataset_name = 'california_housing'\n",
    "results_path = f'logs/{dataset_name}/CNN_Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"../Datasets_benchmark/Regression/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=32):\n",
    "\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(images_folder):\n",
    "        #Generate thet images\n",
    "        image_model.generateImages(df, images_folder)\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    img_paths = os.path.join(images_folder,problem_type+\".csv\")\n",
    "\n",
    "    print(img_paths)\n",
    "    \n",
    "    imgs = pd.read_csv(img_paths)\n",
    "\n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = images_folder + \"/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, df], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    df_x = combined_dataset.drop(df.columns[-1], axis=1).drop(\"values\", axis=1)\n",
    "    df_y = combined_dataset[\"values\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df_x, df_y, test_size=0.20, random_state=SEED)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.50, random_state=SEED)\n",
    "    # Numerical data\n",
    "    X_train_num = X_train.drop(\"images\", axis=1)\n",
    "    X_val_num = X_val.drop(\"images\", axis=1)\n",
    "    X_test_num = X_test.drop(\"images\", axis=1)\n",
    "\n",
    "    # Image data\n",
    "    X_train_img = np.array([cv2.imread(img) for img in X_train[\"images\"]])\n",
    "    X_val_img = np.array([cv2.imread(img) for img in X_val[\"images\"]])\n",
    "    X_test_img = np.array([cv2.imread(img) for img in X_test[\"images\"]])\n",
    "\n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale numerical data\n",
    "    X_train_num = pd.DataFrame(scaler.fit_transform(X_train_num), columns=X_train_num.columns)\n",
    "    X_val_num = pd.DataFrame(scaler.transform(X_val_num), columns=X_val_num.columns)\n",
    "    X_test_num = pd.DataFrame(scaler.transform(X_test_num), columns=X_test_num.columns)\n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "    height, width, channels = X_train_img[0].shape\n",
    "    imgs_shape = (channels, height, width)\n",
    "\n",
    "    print(\"Images shape: \", imgs_shape)\n",
    "    print(\"Attributes: \", attributes)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_num_tensor = torch.tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor = torch.tensor(X_val_num.values, dtype=torch.float32)\n",
    "    X_test_num_tensor = torch.tensor(X_test_num.values, dtype=torch.float32)\n",
    "    X_train_img_tensor = torch.tensor(X_train_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    X_val_img_tensor = torch.tensor(X_val_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    X_test_img_tensor = torch.tensor(X_test_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_num_tensor, X_train_img_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_num_tensor, X_val_img_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_num_tensor, X_test_img_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes, imgs_shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape):\n",
    "        super(Model1, self).__init__()\n",
    "        \n",
    "        # ViT branch\n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape[1],\n",
    "            patch_size = imgs_shape[1],\n",
    "            dim = 64, # This is the output dim of ViT\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "            mlp_dim = 128,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        )\n",
    "        \n",
    "        self.vit_mlp = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # MLP branch\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(attributes, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Final MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(32, 32),  # 16 (ViT) + 16 (MLP) = 32\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, mlp_input, vit_input):\n",
    "        # ViT branch\n",
    "        vit_output = self.vit(vit_input)\n",
    "        vit_output = self.vit_mlp(vit_output)\n",
    "        \n",
    "        # MLP branch\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "        \n",
    "        # Concatenate\n",
    "        concat_output = torch.cat((vit_output, mlp_output), dim=1)\n",
    "        \n",
    "        # Final MLP\n",
    "        final_output = self.final_mlp(concat_output)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT with patch size as divisor of image size\n",
    "# ViT + MLP\n",
    "\n",
    "def find_divisors(n):\n",
    "    divisors = []\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            divisors.append(i)\n",
    "            if i != n // i:  # Check to include both divisors if they are not the same\n",
    "                divisors.append(n // i)\n",
    "    divisors.sort()\n",
    "    return divisors\n",
    "\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape):\n",
    "        super(Model2, self).__init__()\n",
    "\n",
    "        divisors = find_divisors(imgs_shape[1])\n",
    "        \n",
    "        # ViT branch\n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape[1],\n",
    "            patch_size = divisors[-2],\n",
    "            dim = 64, # This is the output dim of ViT\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "            mlp_dim = 128,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        )\n",
    "        \n",
    "        self.vit_mlp = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # MLP branch\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(attributes, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Final MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(32, 32),  # 16 (ViT) + 16 (MLP) = 32\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, mlp_input, vit_input):\n",
    "        # ViT branch\n",
    "        vit_output = self.vit(vit_input)\n",
    "        vit_output = self.vit_mlp(vit_output)\n",
    "        \n",
    "        # MLP branch\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "        \n",
    "        # Concatenate\n",
    "        concat_output = torch.cat((vit_output, mlp_output), dim=1)\n",
    "        \n",
    "        # Final MLP\n",
    "        final_output = self.final_mlp(concat_output)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape):\n",
    "        super(Model3, self).__init__()\n",
    "\n",
    "        divisors = find_divisors(imgs_shape[1])\n",
    "        \n",
    "        # ViT branch\n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape[1],\n",
    "            patch_size = divisors[-3],\n",
    "            dim = 64, # This is the output dim of ViT\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "            mlp_dim = 128,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        )\n",
    "        \n",
    "        self.vit_mlp = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # MLP branch\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(attributes, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Final MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(32, 32),  # 16 (ViT) + 16 (MLP) = 32\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, mlp_input, vit_input):\n",
    "        # ViT branch\n",
    "        vit_output = self.vit(vit_input)\n",
    "        vit_output = self.vit_mlp(vit_output)\n",
    "        \n",
    "        # MLP branch\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "        \n",
    "        # Concatenate\n",
    "        concat_output = torch.cat((vit_output, mlp_output), dim=1)\n",
    "        \n",
    "        # Final MLP\n",
    "        final_output = self.final_mlp(concat_output)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([4, 1])\n",
      "Model forward pass successful!\n",
      "Output shape is correct!\n",
      "Backward pass successful!\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "attributes = 10  # Number of attributes for MLP input\n",
    "img_size = (3, 244, 244)  # Assuming square images\n",
    "model = Model1(attributes, img_size)\n",
    "\n",
    "# Create random input tensors\n",
    "batch_size = 4\n",
    "mlp_input = torch.randn(batch_size, attributes)\n",
    "vit_input = torch.randn(batch_size, 3, 244, 244)\n",
    "\n",
    "# Try to get an output from the model\n",
    "try:\n",
    "    output = model(mlp_input, vit_input)\n",
    "    print(f\"Model output shape: {output.shape}\")\n",
    "    print(\"Model forward pass successful!\")\n",
    "    \n",
    "    # Check if the output shape is as expected (batch_size, 1)\n",
    "    assert output.shape == (batch_size, 1), f\"Expected output shape ({batch_size}, 1), but got {output.shape}\"\n",
    "    print(\"Output shape is correct!\")\n",
    "    \n",
    "    # Check if the model can compute gradients\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    print(\"Backward pass successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPILE AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self ,NumBins=32):\n",
    "        self.InNodes=int(NumBins)*2\n",
    "        self.MediumNode=self.InNodes*2\n",
    "        super(CNN, self).__init__()\n",
    "        self.Lin1 = nn.Linear(self.InNodes , self.MediumNode)\n",
    "        self.Lin2 = nn.Linear(self.MediumNode,  self.MediumNode)\n",
    "        self.Lin5 = nn.Linear(self.MediumNode, 2)\n",
    "    def forward(self, input):\n",
    "        Zoutput = self.Lin1(input)\n",
    "        Zoutput=F.relu(Zoutput)\n",
    "        Zoutput = self.Lin2(Zoutput)\n",
    "        Zoutput=F.relu(Zoutput)\n",
    "        Zoutput = self.Lin5(Zoutput)\n",
    "        return Zoutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU 0.29464295506477356, GPU 0.00992930680513382\n"
     ]
    }
   ],
   "source": [
    "# Create random input tensors\n",
    "batch_size = 4\n",
    "mlp_input = torch.randn(1000, attributes)\n",
    "vit_input = torch.randn(1000, 3, 244, 244)\n",
    "attributes = 10  # Number of attributes for MLP input\n",
    "img_size = (3, 244, 244)  # Assuming square images\n",
    "model = Model1(attributes, img_size)\n",
    "\n",
    "cpu_times = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    t0 = time.perf_counter()\n",
    "    output = model(mlp_input, vit_input)\n",
    "    t1 = time.perf_counter()\n",
    "    cpu_times.append(t1 - t0)\n",
    "\n",
    "device = 'cuda'\n",
    "model = model.to(device)\n",
    "mlp_input = torch.randn(1000, attributes).to(device)\n",
    "vit_input = torch.randn(1000, 3, 244, 244).to(device)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "gpu_times = []\n",
    "for epoch in range(100):\n",
    "    torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    output = model(mlp_input, vit_input)\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    gpu_times.append(t1 - t0)\n",
    "\n",
    "print('CPU {}, GPU {}'.format(\n",
    "    torch.tensor(cpu_times).mean(),\n",
    "    torch.tensor(gpu_times).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=64, epochs=200, lr=1e-3, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = 20\n",
    "    best_model = None\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mse': [], 'val_mse': [], 'train_rmse': [], 'val_rmse': [], 'learning_rate': [], 'epoch_time': []}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "        for num_data, img_data, targets in train_loader:\n",
    "            num_data, img_data, targets = num_data.to(device), img_data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(num_data, img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for num_data, img_data, targets in val_loader:\n",
    "                num_data, img_data, targets = num_data.to(device), img_data.to(device), targets.to(device)\n",
    "                outputs = model(num_data, img_data)\n",
    "                val_loss += loss_fn(outputs, targets).item()\n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "\n",
    "        # Calculate MSE and RMSE\n",
    "        train_mse = mean_squared_error(train_targets, train_predictions)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        val_mse = mean_squared_error(val_targets, val_predictions)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "        history['val_mse'].append(val_mse)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "            f'Train MSE: {train_mse:.4f}, Val MSE: {val_mse:.4f}, '\n",
    "            f'Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}, '\n",
    "            f'Time: {epoch_time:.2f}s')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Calculate and save metrics\n",
    "    train_metrics = calculate_metrics(model, train_loader, device)\n",
    "    val_metrics = calculate_metrics(model, val_loader, device)\n",
    "    test_metrics = calculate_metrics(model, test_loader, device)\n",
    "\n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'train_mse': train_metrics['mse'],\n",
    "        'train_mae': train_metrics['mae'],\n",
    "        'train_rmse': train_metrics['rmse'],\n",
    "        'train_r2': train_metrics['r2'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_mse': val_metrics['mse'],\n",
    "        'val_mae': val_metrics['mae'],\n",
    "        'val_rmse': val_metrics['rmse'],\n",
    "        'val_r2': val_metrics['r2'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_mse': test_metrics['mse'],\n",
    "        'test_mae': test_metrics['mae'],\n",
    "        'test_rmse': test_metrics['rmse'],\n",
    "        'test_r2': test_metrics['r2'],\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "\n",
    "    # Save figures\n",
    "    os.makedirs(f\"models/{dataset_name}/{model_name}\", exist_ok=True)\n",
    "    plot_metric(history['train_loss'], history['val_loss'], 'Loss', dataset_name, model_name)\n",
    "    plot_metric(history['train_mse'], history['val_mse'], 'MSE', dataset_name, model_name)\n",
    "    plot_metric(history['train_rmse'], history['val_rmse'], 'RMSE', dataset_name, model_name)\n",
    "    plot_learning_rate(history['learning_rate'], dataset_name, model_name)\n",
    "\n",
    "    # Save metrics to a file\n",
    "    os.makedirs(f'{results_path}/{model_name}', exist_ok=True)\n",
    "    with open(f'{results_path}/{model_name}/{dataset_name}_metrics.txt', 'w') as f:\n",
    "        for key, value in metrics.items():\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for num_data, img_data, targets in data_loader:\n",
    "            num_data, img_data, targets = num_data.to(device), img_data.to(device), targets.to(device)\n",
    "            outputs = model(num_data, img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_predictions)\n",
    "    mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_predictions)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }    \n",
    "\n",
    "def plot_metric(train_metric, val_metric, metric_name, dataset_name, model_name):\n",
    "    plt.figure()\n",
    "    plt.plot(train_metric, label=f'Train {metric_name}')\n",
    "    plt.plot(val_metric, label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'{metric_name} vs. Epoch')\n",
    "    plt.savefig(f\"models/{dataset_name}/{model_name}/{metric_name.lower()}_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_learning_rate(learning_rates, dataset_name, model_name):\n",
    "    plt.figure()\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate vs. Epoch')\n",
    "    plt.yscale('log')  # Use log scale for better visualization\n",
    "    plt.savefig(f\"models/{dataset_name}/{model_name}/learning_rate_plot.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=32, epochs=200, lr=1e-3, device='cuda'):\n",
    "    try:\n",
    "        if model is None:\n",
    "            print(f\"Model {model_name} is None\")\n",
    "            return None\n",
    "        else:\n",
    "            # Compile and fit the model\n",
    "            metrics = compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, epochs=epochs, lr=lr, device=device)\n",
    "            return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compile and fit {model_name}: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Clear CUDA cache and force garbage collection\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_create_model(model_class, attributes, imgs_shape):\n",
    "    try:\n",
    "        model = model_class(attributes, imgs_shape)\n",
    "        \n",
    "        # Test the model with a sample input\n",
    "        num_input = torch.randn(1, attributes)\n",
    "        img_input = torch.randn(1, *imgs_shape)\n",
    "        output = model(num_input, img_input)\n",
    "        \n",
    "        print(f\"Successfully created and tested {model_class.__name__}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or testing {model_class.__name__}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, random_seed=SEED)\n",
    "#image_model = REFINED(problem= problem_type,hcIterations=5)\n",
    "#image_model = IGTD(problem= problem_type)\n",
    "#image_model = BarGraph(problem= problem_type)\n",
    "#image_model = DistanceMatrix(problem= problem_type)\n",
    "#image_model = Combination(problem= problem_type)\n",
    "#image_model = SuperTML(problem= problem_type)\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_IGTD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default the seed is 1 for all experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 1: TINTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, random_seed=SEED)\n",
    "name = f\"TINTO_blur\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images are already generated\n",
      "../HyNNImages/Regression/california_housing/images_california_housing_TINTO_blur\\regression.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape:  (3, 20, 20)\n",
      "Attributes:  8\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and tested Model1\n",
      "Successfully created and tested Model2\n",
      "Successfully created and tested Model3\n",
      "Epoch 1/200, Train Loss: 1.1073, Val Loss: 0.5745, Train MSE: 1.1073, Val MSE: 0.5762, Train RMSE: 1.0523, Val RMSE: 0.7591, Time: 6.93s\n",
      "Epoch 2/200, Train Loss: 0.5634, Val Loss: 0.6078, Train MSE: 0.5634, Val MSE: 0.6089, Train RMSE: 0.7506, Val RMSE: 0.7803, Time: 6.61s\n",
      "Epoch 3/200, Train Loss: 0.5476, Val Loss: 0.5953, Train MSE: 0.5476, Val MSE: 0.5964, Train RMSE: 0.7400, Val RMSE: 0.7723, Time: 6.47s\n",
      "Epoch 4/200, Train Loss: 0.5218, Val Loss: 0.5167, Train MSE: 0.5218, Val MSE: 0.5184, Train RMSE: 0.7223, Val RMSE: 0.7200, Time: 6.53s\n",
      "Epoch 5/200, Train Loss: 0.4887, Val Loss: 0.4995, Train MSE: 0.4887, Val MSE: 0.5010, Train RMSE: 0.6991, Val RMSE: 0.7078, Time: 6.45s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model3 \u001b[38;5;241m=\u001b[39m try_create_model(Model3, attributes, imgs_shape)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Example usage with two models\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model1_metrics \u001b[38;5;241m=\u001b[39m safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Model1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m model2_metrics \u001b[38;5;241m=\u001b[39m safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Model2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m model3_metrics \u001b[38;5;241m=\u001b[39m safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Model3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[138], line 8\u001b[0m, in \u001b[0;36msafe_compile_and_fit\u001b[1;34m(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size, epochs, lr, device)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# Compile and fit the model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, epochs\u001b[38;5;241m=\u001b[39mepochs, lr\u001b[38;5;241m=\u001b[39mlr, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[137], line 41\u001b[0m, in \u001b[0;36mcompile_and_fit\u001b[1;34m(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size, epochs, lr, device)\u001b[0m\n\u001b[0;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m---> 41\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(num_data, img_data)\n\u001b[0;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, targets)\n\u001b[0;32m     43\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[131], line 47\u001b[0m, in \u001b[0;36mModel1.forward\u001b[1;34m(self, mlp_input, vit_input)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, mlp_input, vit_input):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# ViT branch\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     vit_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit(vit_input)\n\u001b[0;32m     48\u001b[0m     vit_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit_mlp(vit_output)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# MLP branch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[130], line 112\u001b[0m, in \u001b[0;36mViT.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    109\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding[:, :(n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m    110\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m--> 112\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n\u001b[0;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    116\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_latent(x)\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[130], line 70\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attn, ff \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 70\u001b[0m         x \u001b[38;5;241m=\u001b[39m attn(x) \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m     71\u001b[0m         x \u001b[38;5;241m=\u001b[39m ff(x) \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[130], line 45\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m---> 45\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_qkv(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     46\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: rearrange(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m'\u001b[39m, h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads), qkv)\n\u001b[0;32m     48\u001b[0m     dots \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, option=\"maximum\", random_seed=SEED)\n",
    "name = f\"TINTO_blur_maximum\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes \u001b[38;5;241m=\u001b[39m load_and_preprocess_data(images_folder, image_model, problem_type)\n",
      "Cell \u001b[1;32mIn[99], line 13\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[1;34m(images_folder, image_model, problem_type, batch_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_preprocess_data\u001b[39m(images_folder, image_model, problem_type, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Generate the images if the folder does not exist\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(images_folder):\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;66;03m#Generate thet images\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m         image_model\u001b[38;5;241m.\u001b[39mgenerateImages(df, images_folder)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe images are already generated\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\TINTOlib\\tinto.py:531\u001b[0m, in \u001b[0;36mTINTO.generateImages\u001b[1;34m(self, data, folder)\u001b[0m\n\u001b[0;32m    527\u001b[0m Y \u001b[38;5;241m=\u001b[39m array[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    530\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m--> 531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__trainingAlg(X, Y, folder\u001b[38;5;241m=\u001b[39mfolder)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\TINTOlib\\tinto.py:493\u001b[0m, in \u001b[0;36mTINTO.__trainingAlg\u001b[1;34m(self, X, Y, folder)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__areaDelimitation()\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__matrixPositions()\n\u001b[1;32m--> 493\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__createImage(X, Y, folder)\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\TINTOlib\\tinto.py:481\u001b[0m, in \u001b[0;36mTINTO.__createImage\u001b[1;34m(self, X, Y, folder)\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe folder \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is already created...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmatrix:\n\u001b[1;32m--> 481\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__imageSampleFilterSubmatrix(X_scaled, Y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_pixel_caract, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__imageSampleFilter(X_scaled, Y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_pixel_caract, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\TINTOlib\\tinto.py:329\u001b[0m, in \u001b[0;36mTINTO.__imageSampleFilterSubmatrix\u001b[1;34m(self, X, Y, coord, matrix)\u001b[0m\n\u001b[0;32m    327\u001b[0m     imagesRoutesArr\u001b[38;5;241m.\u001b[39mappend(route)\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupervised\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 329\u001b[0m     route \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__saveRegressionOrUnsupervised( i, matrix_a)\n\u001b[0;32m    330\u001b[0m     imagesRoutesArr\u001b[38;5;241m.\u001b[39mappend(route)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\TINTOlib\\tinto.py:288\u001b[0m, in \u001b[0;36mTINTO.__saveRegressionOrUnsupervised\u001b[1;34m(self, i, matrix_a)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Repeat matrix to apply zoom\u001b[39;00m\n\u001b[0;32m    287\u001b[0m matrix_a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(np\u001b[38;5;241m.\u001b[39mrepeat(matrix_a, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzoom, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzoom, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 288\u001b[0m matplotlib\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(route_complete, matrix_a, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mextension, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzoom)\n\u001b[0;32m    290\u001b[0m route_relative \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subfolder, name_image)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m route_relative\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\matplotlib\\image.py:1676\u001b[0m, in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1674\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1675\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[1;32m-> 1676\u001b[0m image\u001b[38;5;241m.\u001b[39msave(fname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\jiayu\\anaconda3\\envs\\TINTO-P\\Lib\\site-packages\\PIL\\Image.py:2436\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2434\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2436\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2439\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, random_seed=SEED)\n",
    "name = f\"TINTO\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 2: IGTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the dataframe\n",
    "num_columns = df.shape[1]\n",
    "\n",
    "# Calculate number of columns - 1\n",
    "columns_minus_one = num_columns - 1\n",
    "\n",
    "# Calculate the square root for image size\n",
    "import math\n",
    "image_size = math.ceil(math.sqrt(columns_minus_one))\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], random_seed=SEED)\n",
    "name = f\"IGTD_{image_size}x{image_size}\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], zoom=2, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size*2}x{image_size*2}\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], zoom=4, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size*4}x{image_size*4}\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], fea_dist_method='Euclidean', image_dist_method='Euclidean', random_seed=SEED)\n",
    "name = f\"IGTD_{image_size}x{image_size}_fEuclidean_iEuclidean\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 3: REFINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = REFINED(problem= problem_type, random_seed=SEED)\n",
    "name = f\"REFINED\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = REFINED(problem= problem_type, zoom=2, random_seed=SEED)\n",
    "name = f\"REFINED_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = REFINED(problem= problem_type, zoom=4, random_seed=SEED)\n",
    "name = f\"REFINED_zoom4\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 4: BAR GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = BarGraph(problem= problem_type)\n",
    "name = f\"BarGraph\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = BarGraph(problem= problem_type, zoom=2)\n",
    "name = f\"BarGraph_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = BarGraph(problem= problem_type, zoom=4)\n",
    "name = f\"BarGraph_zoom4\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 5: DISTANCE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = DistanceMatrix(problem= problem_type)\n",
    "name = f\"DistanceMatrix\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = DistanceMatrix(problem= problem_type, zoom=2)\n",
    "name = f\"DistanceMatrix_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = DistanceMatrix(problem= problem_type, zoom=4)\n",
    "name = f\"DistanceMatrix_zoom4\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 6: COMBINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = Combination(problem= problem_type)\n",
    "name = f\"Combination\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = Combination(problem= problem_type, zoom=2)\n",
    "name = f\"Combination_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = Combination(problem= problem_type, zoom=4)\n",
    "name = f\"Combination_zoom4\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 7: SUPERTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = SuperTML(problem= problem_type, random_seed=SEED)\n",
    "name = f\"SuperTML-EF\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = SuperTML(problem= problem_type, feature_importance=True, font_size=30, random_seed=SEED)\n",
    "name = f\"SuperTML-VF_FS30\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, attributes, imgs_shape)\n",
    "model2 = try_create_model(Model2, attributes, imgs_shape)\n",
    "model3 = try_create_model(Model3, attributes, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TINTO-P",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
