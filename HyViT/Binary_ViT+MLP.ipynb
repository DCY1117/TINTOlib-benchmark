{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "#import cv2\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "#import openslide\n",
    "#from openslide.deepzoom import DeepZoomGenerator\n",
    "import tifffile as tifi\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,mean_absolute_percentage_error\n",
    "\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import vgg16, vgg19, resnet50, mobilenet, inception_resnet_v2, densenet, inception_v3, xception, nasnet, ResNet152V2\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, InputLayer, LayerNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adamax\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanAbsolutePercentageError\n",
    "from tensorflow.keras.layers import Input, Activation,MaxPooling2D, Concatenate, AveragePooling2D, Lambda\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#Models of TINTOlib\n",
    "from TINTOlib.tinto import TINTO\n",
    "from TINTOlib.supertml import SuperTML\n",
    "from TINTOlib.igtd import IGTD\n",
    "from TINTOlib.refined import REFINED\n",
    "from TINTOlib.barGraph import BarGraph\n",
    "from TINTOlib.distanceMatrix import DistanceMatrix\n",
    "from TINTOlib.combination import Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable to store dataset name\n",
    "dataset_name = 'preprocessed_heloc'\n",
    "results_path = f'logs/{dataset_name}/ViT+MLP_Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"../Datasets_benchmark/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ExternalRiskEstimate', 'MSinceOldestTradeOpen',\n",
       "       'MSinceMostRecentTradeOpen', 'AverageMInFile', 'NumSatisfactoryTrades',\n",
       "       'NumTrades60Ever2DerogPubRec', 'NumTrades90Ever2DerogPubRec',\n",
       "       'PercentTradesNeverDelq', 'MSinceMostRecentDelq',\n",
       "       'MaxDelq2PublicRecLast12M', 'MaxDelqEver', 'NumTotalTrades',\n",
       "       'NumTradesOpeninLast12M', 'PercentInstallTrades',\n",
       "       'MSinceMostRecentInqexcl7days', 'NumInqLast6M', 'NumInqLast6Mexcl7days',\n",
       "       'NetFractionRevolvingBurden', 'NetFractionInstallBurden',\n",
       "       'NumRevolvingTradesWBalance', 'NumInstallTradesWBalance',\n",
       "       'NumBank2NatlTradesWHighUtilization', 'PercentTradesWBalance',\n",
       "       'RiskPerformance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LABEL ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "9866    1\n",
      "9867    0\n",
      "9868    0\n",
      "9869    0\n",
      "9870    0\n",
      "Name: RiskPerformance, Length: 9871, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the last column with LabelEncoder\n",
    "df.iloc[:, -1] = label_encoder.fit_transform(df.iloc[:, -1])\n",
    "\n",
    "# Display the updated last column\n",
    "print(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(images_folder, image_model, problem_type):\n",
    "\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(images_folder):\n",
    "        #Generate thet images\n",
    "        image_model.generateImages(df, images_folder)\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    img_paths = os.path.join(images_folder,problem_type+\".csv\")\n",
    "\n",
    "    print(img_paths)\n",
    "    \n",
    "    imgs = pd.read_csv(img_paths)\n",
    "\n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = images_folder + \"/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, df], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    df_x = combined_dataset.drop(df.columns[-1], axis=1).drop(\"class\", axis=1)\n",
    "    df_y = combined_dataset[\"class\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df_x, df_y, test_size=0.40, random_state=SEED)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.50, random_state=SEED)\n",
    "\n",
    "    # Numerical data\n",
    "    X_train_num = X_train.drop(\"images\", axis=1)\n",
    "    X_val_num = X_val.drop(\"images\", axis=1)\n",
    "    X_test_num = X_test.drop(\"images\", axis=1)\n",
    "\n",
    "    # Image data\n",
    "    X_train_img = np.array([cv2.imread(img) for img in X_train[\"images\"]])\n",
    "    X_val_img = np.array([cv2.imread(img) for img in X_val[\"images\"]])\n",
    "    X_test_img = np.array([cv2.imread(img) for img in X_test[\"images\"]])\n",
    "\n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale numerical data\n",
    "    X_train_num = pd.DataFrame(scaler.fit_transform(X_train_num), columns=X_train_num.columns)\n",
    "    X_val_num = pd.DataFrame(scaler.transform(X_val_num), columns=X_val_num.columns)\n",
    "    X_test_num = pd.DataFrame(scaler.transform(X_test_num), columns=X_test_num.columns)\n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "    imgs_shape = X_train_img[0].shape\n",
    "\n",
    "    print(\"Images shape: \",imgs_shape)\n",
    "    print(\"Attributres: \",attributes)\n",
    "    pixels=X_train_img[0].shape[0]\n",
    "    print(\"Image size (pixels):\", pixels)\n",
    "\n",
    "    return X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as nn\n",
    "\n",
    "from tensorflow import einsum\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class PreNorm(Layer):\n",
    "    def __init__(self, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "\n",
    "        self.norm = nn.LayerNormalization()\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.fn(self.norm(x), training=training)\n",
    "\n",
    "class MLP(Layer):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.0):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        def GELU():\n",
    "            def gelu(x, approximate=False):\n",
    "                if approximate:\n",
    "                    coeff = tf.cast(0.044715, x.dtype)\n",
    "                    return 0.5 * x * (1.0 + tf.tanh(0.7978845608028654 * (x + coeff * tf.pow(x, 3))))\n",
    "                else:\n",
    "                    return 0.5 * x * (1.0 + tf.math.erf(x / tf.cast(1.4142135623730951, x.dtype)))\n",
    "\n",
    "            return nn.Activation(gelu)\n",
    "\n",
    "        self.net = Sequential([\n",
    "            nn.Dense(units=hidden_dim),\n",
    "            GELU(),\n",
    "            nn.Dropout(rate=dropout),\n",
    "            nn.Dense(units=dim),\n",
    "            nn.Dropout(rate=dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.net(x, training=training)\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.0):\n",
    "        super(Attention, self).__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax()\n",
    "        self.to_qkv = nn.Dense(units=inner_dim * 3, use_bias=False)\n",
    "\n",
    "        if project_out:\n",
    "            self.to_out = [\n",
    "                nn.Dense(units=dim),\n",
    "                nn.Dropout(rate=dropout)\n",
    "            ]\n",
    "        else:\n",
    "            self.to_out = []\n",
    "\n",
    "        self.to_out = Sequential(self.to_out)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        # dots = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) * self.scale\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        # x = tf.matmul(attn, v)\n",
    "        x = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        x = rearrange(x, 'b h n d -> b n (h d)')\n",
    "        x = self.to_out(x, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(Layer):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.0):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append([\n",
    "                PreNorm(Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(MLP(dim, mlp_dim, dropout=dropout))\n",
    "            ])\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        for attn, mlp in self.layers:\n",
    "            x = attn(x, training=training) + x\n",
    "            x = mlp(x, training=training) + x\n",
    "\n",
    "        return x\n",
    "\n",
    "class ViT(Model):\n",
    "    def __init__(self, image_size, patch_size, dim, depth, heads, mlp_dim,\n",
    "                 pool='cls', dim_head=64, dropout=0.0, emb_dropout=0.0):\n",
    "        \"\"\"\n",
    "            image_size: int.\n",
    "            -> Image size. If you have rectangular images, make sure your image size is the maximum of the width and height\n",
    "            patch_size: int.\n",
    "            -> Number of patches. image_size must be divisible by patch_size.\n",
    "            -> The number of patches is: n = (image_size // patch_size) ** 2 and n must be greater than 16.\n",
    "            num_classes: int.\n",
    "            -> Number of classes to classify.\n",
    "            dim: int.\n",
    "            -> Last dimension of output tensor after linear transformation nn.Linear(..., dim).\n",
    "            depth: int.\n",
    "            -> Number of Transformer blocks.\n",
    "            heads: int.\n",
    "            -> Number of heads in Multi-head Attention layer.\n",
    "            mlp_dim: int.\n",
    "            -> Dimension of the MLP (FeedForward) layer.\n",
    "            dropout: float between [0, 1], default 0..\n",
    "            -> Dropout rate.\n",
    "            emb_dropout: float between [0, 1], default 0.\n",
    "            -> Embedding dropout rate.\n",
    "            pool: string, either cls token pooling or mean pooling\n",
    "        \"\"\"\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.patch_embedding = Sequential([\n",
    "            Rearrange('b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
    "            nn.Dense(units=dim)\n",
    "        ], name='patch_embedding')\n",
    "\n",
    "        self.pos_embedding = tf.Variable(initial_value=tf.random.normal([1, num_patches + 1, dim]))\n",
    "        self.cls_token = tf.Variable(initial_value=tf.random.normal([1, 1, dim]))\n",
    "        self.dropout = nn.Dropout(rate=emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "\n",
    "    def call(self, img, training=True, **kwargs):\n",
    "        x = self.patch_embedding(img)\n",
    "        b, n, d = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
    "        x = tf.concat([cls_tokens, x], axis=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        x = self.transformer(x, training=training)\n",
    "\n",
    "        if self.pool == 'mean':\n",
    "            x = tf.reduce_mean(x, axis=1)\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT + MLP\n",
    "\n",
    "def create_model1(imgs_shape, attributes):\n",
    "    # ViT branch\n",
    "    vit_input = Input(shape=imgs_shape)\n",
    "\n",
    "    vit_model = ViT(\n",
    "        image_size = imgs_shape[0],\n",
    "        patch_size = imgs_shape[0],\n",
    "        dim = 64,\n",
    "        depth = 2,\n",
    "        heads = 4,\n",
    "        mlp_dim = 128,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "    )\n",
    "\n",
    "    # Wrap the ViT model call in a Lambda layer\n",
    "    vit_output = Lambda(lambda x: vit_model(x), output_shape=(64,))(vit_input)\n",
    "\n",
    "    vit_output = Dense(32, activation='relu')(vit_output)\n",
    "    vit_output = Dense(16, activation='relu')(vit_output)\n",
    "\n",
    "    # MLP branch\n",
    "    mlp_input = Input(shape=(attributes,))\n",
    "    mlp_output = Dense(16, activation='relu')(mlp_input)\n",
    "    mlp_output = Dense(32, activation='relu')(mlp_output)\n",
    "    mlp_output = Dense(16, activation='relu')(mlp_output)\n",
    "\n",
    "    # Concatenate the outputs of CNN and MLP branches\n",
    "    concat_output = Concatenate()([vit_output, mlp_output])\n",
    "\n",
    "    # Final MLP\n",
    "    final_output = Dense(32, activation='relu')(concat_output)\n",
    "    final_output = Dense(16, activation='relu')(final_output)\n",
    "    final_output = Dense(8, activation='relu')(final_output)\n",
    "    final_output = Dense(1, activation='sigmoid')(final_output)  # Output layer for regression\n",
    "\n",
    "    # Create the hybrid model\n",
    "    model1 = Model(inputs=[mlp_input, vit_input], outputs=final_output)\n",
    "\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT with patch size as divisor of image size\n",
    "# ViT + MLP\n",
    "\n",
    "def find_divisors(n):\n",
    "    divisors = []\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            divisors.append(i)\n",
    "            if i != n // i:  # Check to include both divisors if they are not the same\n",
    "                divisors.append(n // i)\n",
    "    divisors.sort()\n",
    "    return divisors\n",
    "\n",
    "def create_model2(imgs_shape, attributes):\n",
    "    divisors = find_divisors(imgs_shape[0])\n",
    "    \n",
    "    vit_input = Input(shape=imgs_shape)\n",
    "\n",
    "    vit_model = ViT(\n",
    "        image_size = imgs_shape[0],\n",
    "        patch_size = divisors[-2],\n",
    "        dim = 64,\n",
    "        depth = 2,\n",
    "        heads = 4,\n",
    "        mlp_dim = 128,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "    )\n",
    "\n",
    "    # Wrap the ViT model call in a Lambda layer\n",
    "    vit_output = Lambda(lambda x: vit_model(x), output_shape=(64,))(vit_input)\n",
    "\n",
    "    vit_output = Dense(64, activation='relu')(vit_output)\n",
    "    vit_output = Dense(32, activation='relu')(vit_output)\n",
    "    vit_output = Dense(16, activation='relu')(vit_output)\n",
    "\n",
    "    # MLP branch\n",
    "    mlp_input = Input(shape=(attributes,))\n",
    "    mlp_output = Dense(16, activation='relu')(mlp_input)\n",
    "    mlp_output = Dense(32, activation='relu')(mlp_output)\n",
    "    mlp_output = Dense(16, activation='relu')(mlp_output)\n",
    "\n",
    "    # Concatenate the outputs of CNN and MLP branches\n",
    "    concat_output = Concatenate()([vit_output, mlp_output])\n",
    "\n",
    "    # Final MLP\n",
    "    final_output = Dense(32, activation='relu')(concat_output)\n",
    "    final_output = Dense(16, activation='relu')(final_output)\n",
    "    final_output = Dense(8, activation='relu')(final_output)\n",
    "    final_output = Dense(1, activation='sigmoid')(final_output)  # Output layer for regression\n",
    "\n",
    "    # Create the hybrid model\n",
    "    model2 = Model(inputs=[mlp_input, vit_input], outputs=final_output)\n",
    "\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    #tf.keras.metrics.TruePositives(name = 'tp'),\n",
    "    #tf.keras.metrics.FalsePositives(name = 'fp'),\n",
    "    #tf.keras.metrics.TrueNegatives(name = 'tn'),\n",
    "    #tf.keras.metrics.FalseNegatives(name = 'fn'), \n",
    "    tf.keras.metrics.BinaryAccuracy(name ='accuracy'),\n",
    "    tf.keras.metrics.Precision(name = 'precision'),\n",
    "    tf.keras.metrics.Recall(name = 'recall'),\n",
    "    tf.keras.metrics.AUC(name = 'auc'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPILE AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def compile_and_fit(model, X_train_num, X_train_img, y_train, X_val_num, X_val_img, y_val, X_test_num, X_test_img, y_test, dataset_name, model_name, batch_size=32, epochs=200, lr=1e-3):\n",
    "\n",
    "    opt = Adam(learning_rate=lr)\n",
    "\n",
    "    # Define the early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',  # Monitor the validation loss\n",
    "        min_delta=0.001,     # Minimum change in the monitored quantity to qualify as an improvement\n",
    "        patience=20,          # Number of epochs with no improvement after which training will be stopped\n",
    "        verbose=1,           # Print messages when the callback takes an action\n",
    "        mode='min',           # Training will stop when the quantity monitored has stopped decreasing\n",
    "        restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    "    )\n",
    "\n",
    "    \n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=opt,\n",
    "        metrics=METRICS\n",
    "    )\n",
    "\n",
    "    model_history = model.fit(\n",
    "        x=[X_train_num,X_train_img], y=y_train,\n",
    "        validation_data=([X_val_num,X_val_img], y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    os.makedirs(f\"models/{dataset_name}/{model_name}\", exist_ok=True)\n",
    "\n",
    "    plt.figure()  # Start a new figure\n",
    "    plt.plot(model_history.history['loss'], color = 'red', label = 'loss')\n",
    "    plt.plot(model_history.history['val_loss'], color = 'green', label = 'val loss')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.savefig(f\"models/{dataset_name}/{model_name}/loss_plot.png\")\n",
    "\n",
    "    plt.figure()  # Start a new figure\n",
    "    plt.plot(model_history.history['accuracy'], color = 'red', label = 'accuracy')\n",
    "    plt.plot(model_history.history['val_accuracy'], color = 'green', label = 'val accuracy')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.savefig(f\"models/{dataset_name}/{model_name}/accuracy_plot.png\")\n",
    "\n",
    "    # Save the model\n",
    "    os.makedirs(f\"models/{dataset_name}/{model_name}\", exist_ok=True)\n",
    "    model.save(f\"models/{dataset_name}/{model_name}/model_{dataset_name}.keras\")\n",
    "\n",
    "    # Evaluate the model on the training set\n",
    "    train_scores = model.evaluate([X_train_num,X_train_img], y_train)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_scores = model.evaluate([X_val_num,X_val_img], y_val)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    score_test = model.evaluate([X_test_num,X_test_img], y_test)\n",
    "\n",
    "    # Save training, validation, and test scores\n",
    "    metrics = {\n",
    "        'train_loss': train_scores[0],\n",
    "        'train_accuracy': train_scores[1],\n",
    "        'train_precision': train_scores[2],\n",
    "        'train_recall': train_scores[3],\n",
    "        'train_auc': train_scores[4],\n",
    "        'val_loss': val_scores[0],\n",
    "        'val_accuracy': val_scores[1],\n",
    "        'val_precision': val_scores[2],\n",
    "        'val_recall': val_scores[3],\n",
    "        'val_auc': val_scores[4],\n",
    "        'test_loss': score_test[0],\n",
    "        'test_accuracy': score_test[1],\n",
    "        'test_precision': score_test[2],\n",
    "        'test_recall': score_test[3],\n",
    "        'test_auc': score_test[4]\n",
    "    }\n",
    "\n",
    "    # Save metrics to a file\n",
    "    os.makedirs(f'{results_path}/{model_name}', exist_ok=True)\n",
    "    with open(f'{results_path}/{model_name}/{dataset_name}_metrics.txt', 'w') as f:\n",
    "        for key, value in metrics.items():\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_compile_and_fit(model, X_train_num, X_train_img, y_train, X_val_num, X_val_img, y_val, X_test_num, X_test_img, y_test,dataset_name, model_name, batch_size=32, epochs=200, lr=1e-3):\n",
    "    try:\n",
    "        metrics = compile_and_fit(model, X_train_num, X_train_img, y_train, X_val_num, X_val_img, y_val, X_test_num, X_test_img, y_test,dataset_name, model_name, batch_size, epochs, lr)\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compile and fit {model_name}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "#image_model = REFINED(problem= problem_type,hcIterations=5)\n",
    "image_model = TINTO(problem= problem_type, blur=True)\n",
    "#image_model = IGTD(problem= problem_type)\n",
    "#image_model = BarGraph(problem= problem_type)\n",
    "#image_model = DistanceMatrix(problem= problem_type)\n",
    "#image_model = Combination(problem= problem_type)\n",
    "#image_model = SuperTML(problem= problem_type)\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Binary/{dataset_name}/images_{dataset_name}_IGTD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = IGTD(problem= problem_type, scale=[3,3])\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Binary/{dataset_name}/images_{dataset_name}_IGTD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images are already generated\n",
      "../HyNNImages/Binary/preprocessed_heloc/images_preprocessed_heloc_IGTD\\supervised.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape:  (5, 5, 3)\n",
      "Attributres:  23\n",
      "Image size (pixels): 5\n"
     ]
    }
   ],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = create_model1(imgs_shape, attributes)\n",
    "model2 = create_model2(imgs_shape, attributes)\n",
    "#model3 = create_model3(imgs_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5922, 5, 5, 3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train_num, X_val_num, X_test_num are NumPy arrays or similar for numerical data\n",
    "X_train_num = np.delete(X_train_num, [0, 1], axis=0)\n",
    "X_test_num = np.delete(X_test_num, [0, 1], axis=0)\n",
    "\n",
    "# Assuming X_train_img, X_val_img, X_test_img are arrays of image data\n",
    "X_train_img = np.delete(X_train_img, [0, 1], axis=0)\n",
    "X_test_img = np.delete(X_test_img, [0, 1], axis=0)\n",
    "\n",
    "# Assuming y_train, y_val, y_test are label arrays\n",
    "y_train = np.delete(y_train, [0, 1], axis=0)\n",
    "y_test = np.delete(y_test, [0, 1], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first 22 rows from the numerical validation data\n",
    "X_val_num = np.delete(X_val_num, np.arange(22), axis=0)\n",
    "\n",
    "# Remove the first 22 rows from the image validation data\n",
    "X_val_img = np.delete(X_val_img, np.arange(22), axis=0)\n",
    "\n",
    "# Remove the first 22 rows from the validation labels\n",
    "y_val = np.delete(y_val, np.arange(22), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5920, 5, 5, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1952, 5, 5, 3)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.5532 - auc: 0.5674 - loss: 0.6832 - precision: 0.5483 - recall: 0.5278 - val_accuracy: 0.6957 - val_auc: 0.7463 - val_loss: 0.5968 - val_precision: 0.6855 - val_recall: 0.6228\n",
      "Epoch 2/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6913 - auc: 0.7547 - loss: 0.5897 - precision: 0.6907 - recall: 0.6756 - val_accuracy: 0.7111 - val_auc: 0.7727 - val_loss: 0.5717 - val_precision: 0.6962 - val_recall: 0.6574\n",
      "Epoch 3/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7095 - auc: 0.7715 - loss: 0.5741 - precision: 0.7083 - recall: 0.6960 - val_accuracy: 0.7188 - val_auc: 0.7785 - val_loss: 0.5653 - val_precision: 0.7108 - val_recall: 0.6529\n",
      "Epoch 4/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7072 - auc: 0.7762 - loss: 0.5693 - precision: 0.7061 - recall: 0.6942 - val_accuracy: 0.7213 - val_auc: 0.7816 - val_loss: 0.5633 - val_precision: 0.6960 - val_recall: 0.6975\n",
      "Epoch 5/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7178 - auc: 0.7822 - loss: 0.5632 - precision: 0.7143 - recall: 0.7106 - val_accuracy: 0.7223 - val_auc: 0.7839 - val_loss: 0.5603 - val_precision: 0.6980 - val_recall: 0.6964\n",
      "Epoch 6/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7173 - auc: 0.7847 - loss: 0.5604 - precision: 0.7156 - recall: 0.7059 - val_accuracy: 0.7254 - val_auc: 0.7858 - val_loss: 0.5585 - val_precision: 0.6991 - val_recall: 0.7054\n",
      "Epoch 7/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7205 - auc: 0.7859 - loss: 0.5592 - precision: 0.7172 - recall: 0.7132 - val_accuracy: 0.7218 - val_auc: 0.7870 - val_loss: 0.5602 - val_precision: 0.6818 - val_recall: 0.7388\n",
      "Epoch 8/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7210 - auc: 0.7886 - loss: 0.5565 - precision: 0.7130 - recall: 0.7243 - val_accuracy: 0.7244 - val_auc: 0.7879 - val_loss: 0.5595 - val_precision: 0.6849 - val_recall: 0.7400\n",
      "Epoch 9/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7238 - auc: 0.7912 - loss: 0.5538 - precision: 0.7155 - recall: 0.7286 - val_accuracy: 0.7213 - val_auc: 0.7893 - val_loss: 0.5556 - val_precision: 0.6901 - val_recall: 0.7132\n",
      "Epoch 10/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7240 - auc: 0.7905 - loss: 0.5547 - precision: 0.7175 - recall: 0.7236 - val_accuracy: 0.7244 - val_auc: 0.7903 - val_loss: 0.5546 - val_precision: 0.6941 - val_recall: 0.7143\n",
      "Epoch 11/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7263 - auc: 0.7939 - loss: 0.5512 - precision: 0.7210 - recall: 0.7235 - val_accuracy: 0.7228 - val_auc: 0.7909 - val_loss: 0.5535 - val_precision: 0.6931 - val_recall: 0.7109\n",
      "Epoch 12/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7226 - auc: 0.7936 - loss: 0.5515 - precision: 0.7178 - recall: 0.7184 - val_accuracy: 0.7249 - val_auc: 0.7918 - val_loss: 0.5534 - val_precision: 0.6932 - val_recall: 0.7188\n",
      "Epoch 13/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7253 - auc: 0.7952 - loss: 0.5495 - precision: 0.7191 - recall: 0.7247 - val_accuracy: 0.7239 - val_auc: 0.7926 - val_loss: 0.5546 - val_precision: 0.6877 - val_recall: 0.7299\n",
      "Epoch 14/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7273 - auc: 0.7955 - loss: 0.5495 - precision: 0.7197 - recall: 0.7299 - val_accuracy: 0.7264 - val_auc: 0.7928 - val_loss: 0.5525 - val_precision: 0.6934 - val_recall: 0.7243\n",
      "Epoch 15/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7247 - auc: 0.7968 - loss: 0.5480 - precision: 0.7177 - recall: 0.7256 - val_accuracy: 0.7239 - val_auc: 0.7931 - val_loss: 0.5540 - val_precision: 0.6881 - val_recall: 0.7288\n",
      "Epoch 16/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7255 - auc: 0.7977 - loss: 0.5470 - precision: 0.7201 - recall: 0.7228 - val_accuracy: 0.7239 - val_auc: 0.7937 - val_loss: 0.5523 - val_precision: 0.6897 - val_recall: 0.7243\n",
      "Epoch 17/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7257 - auc: 0.7986 - loss: 0.5461 - precision: 0.7191 - recall: 0.7262 - val_accuracy: 0.7239 - val_auc: 0.7937 - val_loss: 0.5527 - val_precision: 0.6897 - val_recall: 0.7243\n",
      "Epoch 18/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7273 - auc: 0.7990 - loss: 0.5457 - precision: 0.7226 - recall: 0.7232 - val_accuracy: 0.7244 - val_auc: 0.7943 - val_loss: 0.5524 - val_precision: 0.6892 - val_recall: 0.7277\n",
      "Epoch 19/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7270 - auc: 0.7997 - loss: 0.5450 - precision: 0.7218 - recall: 0.7242 - val_accuracy: 0.7244 - val_auc: 0.7944 - val_loss: 0.5516 - val_precision: 0.6896 - val_recall: 0.7266\n",
      "Epoch 20/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7278 - auc: 0.8016 - loss: 0.5429 - precision: 0.7230 - recall: 0.7242 - val_accuracy: 0.7244 - val_auc: 0.7944 - val_loss: 0.5517 - val_precision: 0.6892 - val_recall: 0.7277\n",
      "Epoch 21/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7269 - auc: 0.8012 - loss: 0.5433 - precision: 0.7216 - recall: 0.7242 - val_accuracy: 0.7239 - val_auc: 0.7946 - val_loss: 0.5530 - val_precision: 0.6865 - val_recall: 0.7333\n",
      "Epoch 22/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7258 - auc: 0.8009 - loss: 0.5437 - precision: 0.7190 - recall: 0.7266 - val_accuracy: 0.7239 - val_auc: 0.7946 - val_loss: 0.5522 - val_precision: 0.6881 - val_recall: 0.7288\n",
      "Epoch 23/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7249 - auc: 0.8017 - loss: 0.5427 - precision: 0.7180 - recall: 0.7268 - val_accuracy: 0.7228 - val_auc: 0.7950 - val_loss: 0.5526 - val_precision: 0.6855 - val_recall: 0.7321\n",
      "Epoch 24/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7264 - auc: 0.8018 - loss: 0.5425 - precision: 0.7222 - recall: 0.7215 - val_accuracy: 0.7213 - val_auc: 0.7951 - val_loss: 0.5532 - val_precision: 0.6833 - val_recall: 0.7321\n",
      "Epoch 25/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7268 - auc: 0.8025 - loss: 0.5421 - precision: 0.7197 - recall: 0.7290 - val_accuracy: 0.7234 - val_auc: 0.7950 - val_loss: 0.5521 - val_precision: 0.6874 - val_recall: 0.7288\n",
      "Epoch 26/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7277 - auc: 0.8024 - loss: 0.5419 - precision: 0.7191 - recall: 0.7333 - val_accuracy: 0.7223 - val_auc: 0.7958 - val_loss: 0.5504 - val_precision: 0.6883 - val_recall: 0.7221\n",
      "Epoch 27/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7299 - auc: 0.8025 - loss: 0.5417 - precision: 0.7208 - recall: 0.7364 - val_accuracy: 0.7239 - val_auc: 0.7951 - val_loss: 0.5525 - val_precision: 0.6889 - val_recall: 0.7266\n",
      "Epoch 28/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7278 - auc: 0.8040 - loss: 0.5401 - precision: 0.7239 - recall: 0.7222 - val_accuracy: 0.7213 - val_auc: 0.7957 - val_loss: 0.5518 - val_precision: 0.6849 - val_recall: 0.7277\n",
      "Epoch 29/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7284 - auc: 0.8038 - loss: 0.5401 - precision: 0.7216 - recall: 0.7296 - val_accuracy: 0.7218 - val_auc: 0.7948 - val_loss: 0.5523 - val_precision: 0.6856 - val_recall: 0.7277\n",
      "Epoch 30/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7277 - auc: 0.8038 - loss: 0.5404 - precision: 0.7203 - recall: 0.7304 - val_accuracy: 0.7223 - val_auc: 0.7949 - val_loss: 0.5514 - val_precision: 0.6879 - val_recall: 0.7232\n",
      "Epoch 31/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7274 - auc: 0.8036 - loss: 0.5404 - precision: 0.7179 - recall: 0.7350 - val_accuracy: 0.7259 - val_auc: 0.7954 - val_loss: 0.5504 - val_precision: 0.6943 - val_recall: 0.7199\n",
      "Epoch 32/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7322 - auc: 0.8041 - loss: 0.5400 - precision: 0.7264 - recall: 0.7313 - val_accuracy: 0.7228 - val_auc: 0.7954 - val_loss: 0.5511 - val_precision: 0.6894 - val_recall: 0.7210\n",
      "Epoch 33/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7295 - auc: 0.8048 - loss: 0.5391 - precision: 0.7240 - recall: 0.7277 - val_accuracy: 0.7218 - val_auc: 0.7954 - val_loss: 0.5506 - val_precision: 0.6888 - val_recall: 0.7188\n",
      "Epoch 34/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7313 - auc: 0.8051 - loss: 0.5389 - precision: 0.7272 - recall: 0.7267 - val_accuracy: 0.7254 - val_auc: 0.7955 - val_loss: 0.5496 - val_precision: 0.6940 - val_recall: 0.7188\n",
      "Epoch 35/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7314 - auc: 0.8056 - loss: 0.5382 - precision: 0.7265 - recall: 0.7283 - val_accuracy: 0.7228 - val_auc: 0.7959 - val_loss: 0.5502 - val_precision: 0.6894 - val_recall: 0.7210\n",
      "Epoch 36/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7307 - auc: 0.8052 - loss: 0.5384 - precision: 0.7250 - recall: 0.7297 - val_accuracy: 0.7234 - val_auc: 0.7955 - val_loss: 0.5508 - val_precision: 0.6890 - val_recall: 0.7243\n",
      "Epoch 37/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7320 - auc: 0.8061 - loss: 0.5376 - precision: 0.7276 - recall: 0.7277 - val_accuracy: 0.7193 - val_auc: 0.7959 - val_loss: 0.5515 - val_precision: 0.6816 - val_recall: 0.7288\n",
      "Epoch 38/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7296 - auc: 0.8064 - loss: 0.5371 - precision: 0.7235 - recall: 0.7295 - val_accuracy: 0.7223 - val_auc: 0.7961 - val_loss: 0.5506 - val_precision: 0.6879 - val_recall: 0.7232\n",
      "Epoch 39/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7292 - auc: 0.8065 - loss: 0.5369 - precision: 0.7235 - recall: 0.7283 - val_accuracy: 0.7254 - val_auc: 0.7959 - val_loss: 0.5494 - val_precision: 0.6948 - val_recall: 0.7165\n",
      "Epoch 40/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7327 - auc: 0.8069 - loss: 0.5365 - precision: 0.7261 - recall: 0.7337 - val_accuracy: 0.7254 - val_auc: 0.7959 - val_loss: 0.5495 - val_precision: 0.6944 - val_recall: 0.7176\n",
      "Epoch 41/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7305 - auc: 0.8071 - loss: 0.5362 - precision: 0.7232 - recall: 0.7329 - val_accuracy: 0.7223 - val_auc: 0.7957 - val_loss: 0.5492 - val_precision: 0.6911 - val_recall: 0.7143\n",
      "Epoch 42/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7326 - auc: 0.8072 - loss: 0.5360 - precision: 0.7264 - recall: 0.7325 - val_accuracy: 0.7234 - val_auc: 0.7962 - val_loss: 0.5496 - val_precision: 0.6922 - val_recall: 0.7154\n",
      "Epoch 43/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7317 - auc: 0.8076 - loss: 0.5357 - precision: 0.7261 - recall: 0.7304 - val_accuracy: 0.7239 - val_auc: 0.7960 - val_loss: 0.5493 - val_precision: 0.6934 - val_recall: 0.7143\n",
      "Epoch 44/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7325 - auc: 0.8079 - loss: 0.5352 - precision: 0.7287 - recall: 0.7274 - val_accuracy: 0.7218 - val_auc: 0.7957 - val_loss: 0.5502 - val_precision: 0.6904 - val_recall: 0.7143\n",
      "Epoch 45/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7320 - auc: 0.8088 - loss: 0.5342 - precision: 0.7277 - recall: 0.7281 - val_accuracy: 0.7172 - val_auc: 0.7956 - val_loss: 0.5523 - val_precision: 0.6807 - val_recall: 0.7232\n",
      "Epoch 46/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7315 - auc: 0.8092 - loss: 0.5334 - precision: 0.7257 - recall: 0.7310 - val_accuracy: 0.7244 - val_auc: 0.7959 - val_loss: 0.5498 - val_precision: 0.6929 - val_recall: 0.7176\n",
      "Epoch 47/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7319 - auc: 0.8088 - loss: 0.5338 - precision: 0.7279 - recall: 0.7271 - val_accuracy: 0.7188 - val_auc: 0.7960 - val_loss: 0.5514 - val_precision: 0.6840 - val_recall: 0.7199\n",
      "Epoch 48/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7309 - auc: 0.8098 - loss: 0.5329 - precision: 0.7274 - recall: 0.7252 - val_accuracy: 0.7223 - val_auc: 0.7954 - val_loss: 0.5504 - val_precision: 0.6907 - val_recall: 0.7154\n",
      "Epoch 49/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7311 - auc: 0.8095 - loss: 0.5330 - precision: 0.7251 - recall: 0.7305 - val_accuracy: 0.7228 - val_auc: 0.7958 - val_loss: 0.5503 - val_precision: 0.6923 - val_recall: 0.7132\n",
      "Epoch 50/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7291 - auc: 0.8093 - loss: 0.5328 - precision: 0.7252 - recall: 0.7245 - val_accuracy: 0.7223 - val_auc: 0.7957 - val_loss: 0.5497 - val_precision: 0.6924 - val_recall: 0.7109\n",
      "Epoch 51/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7312 - auc: 0.8096 - loss: 0.5324 - precision: 0.7275 - recall: 0.7255 - val_accuracy: 0.7254 - val_auc: 0.7948 - val_loss: 0.5506 - val_precision: 0.6974 - val_recall: 0.7098\n",
      "Epoch 52/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7307 - auc: 0.8101 - loss: 0.5319 - precision: 0.7266 - recall: 0.7263 - val_accuracy: 0.7254 - val_auc: 0.7955 - val_loss: 0.5499 - val_precision: 0.6974 - val_recall: 0.7098\n",
      "Epoch 53/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7318 - auc: 0.8106 - loss: 0.5314 - precision: 0.7270 - recall: 0.7290 - val_accuracy: 0.7254 - val_auc: 0.7951 - val_loss: 0.5505 - val_precision: 0.6969 - val_recall: 0.7109\n",
      "Epoch 54/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7324 - auc: 0.8104 - loss: 0.5313 - precision: 0.7276 - recall: 0.7294 - val_accuracy: 0.7223 - val_auc: 0.7946 - val_loss: 0.5515 - val_precision: 0.6932 - val_recall: 0.7087\n",
      "Epoch 55/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7301 - auc: 0.8109 - loss: 0.5308 - precision: 0.7258 - recall: 0.7261 - val_accuracy: 0.7193 - val_auc: 0.7948 - val_loss: 0.5523 - val_precision: 0.6839 - val_recall: 0.7221\n",
      "Epoch 56/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7338 - auc: 0.8117 - loss: 0.5297 - precision: 0.7277 - recall: 0.7341 - val_accuracy: 0.7223 - val_auc: 0.7945 - val_loss: 0.5512 - val_precision: 0.6937 - val_recall: 0.7076\n",
      "Epoch 57/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7320 - auc: 0.8118 - loss: 0.5297 - precision: 0.7275 - recall: 0.7283 - val_accuracy: 0.7193 - val_auc: 0.7942 - val_loss: 0.5530 - val_precision: 0.6863 - val_recall: 0.7154\n",
      "Epoch 58/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7321 - auc: 0.8122 - loss: 0.5294 - precision: 0.7274 - recall: 0.7293 - val_accuracy: 0.7172 - val_auc: 0.7942 - val_loss: 0.5533 - val_precision: 0.6830 - val_recall: 0.7165\n",
      "Epoch 59/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7343 - auc: 0.8126 - loss: 0.5288 - precision: 0.7301 - recall: 0.7305 - val_accuracy: 0.7228 - val_auc: 0.7938 - val_loss: 0.5530 - val_precision: 0.6890 - val_recall: 0.7221\n",
      "Epoch 59: early stopping\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7324 - auc: 0.8025 - loss: 0.5421 - precision: 0.7197 - recall: 0.7475\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7152 - auc: 0.7880 - loss: 0.5583 - precision: 0.6730 - recall: 0.7146\n",
      "Failed to compile and fit IGTD_model1: Exception encountered when calling ViT.call().\n",
      "\n",
      "\u001b[1munsupported operand type(s) for *=: 'int' and 'NoneType'\u001b[0m\n",
      "\n",
      "Arguments received by ViT.call():\n",
      "  • img=tf.Tensor(shape=(None, 5, 5, 3), dtype=float32)\n",
      "  • training=False\n",
      "  • kwargs=<class 'inspect._empty'>\n",
      "Epoch 1/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - accuracy: 0.5865 - auc: 0.6298 - loss: 0.6673 - precision: 0.5875 - recall: 0.5298 - val_accuracy: 0.6942 - val_auc: 0.7663 - val_loss: 0.6008 - val_precision: 0.6327 - val_recall: 0.7958\n",
      "Epoch 2/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.6869 - auc: 0.7597 - loss: 0.5855 - precision: 0.6819 - recall: 0.6820 - val_accuracy: 0.7054 - val_auc: 0.7724 - val_loss: 0.5770 - val_precision: 0.7137 - val_recall: 0.5982\n",
      "Epoch 3/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7056 - auc: 0.7705 - loss: 0.5752 - precision: 0.7021 - recall: 0.6982 - val_accuracy: 0.7147 - val_auc: 0.7780 - val_loss: 0.5664 - val_precision: 0.7040 - val_recall: 0.6529\n",
      "Epoch 4/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7082 - auc: 0.7770 - loss: 0.5684 - precision: 0.7043 - recall: 0.7022 - val_accuracy: 0.7147 - val_auc: 0.7821 - val_loss: 0.5630 - val_precision: 0.7070 - val_recall: 0.6462\n",
      "Epoch 5/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7180 - auc: 0.7822 - loss: 0.5637 - precision: 0.7157 - recall: 0.7080 - val_accuracy: 0.7208 - val_auc: 0.7859 - val_loss: 0.5590 - val_precision: 0.7087 - val_recall: 0.6652\n",
      "Epoch 6/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7183 - auc: 0.7847 - loss: 0.5609 - precision: 0.7168 - recall: 0.7072 - val_accuracy: 0.7223 - val_auc: 0.7882 - val_loss: 0.5575 - val_precision: 0.7174 - val_recall: 0.6518\n",
      "Epoch 7/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7187 - auc: 0.7859 - loss: 0.5598 - precision: 0.7198 - recall: 0.7015 - val_accuracy: 0.7213 - val_auc: 0.7902 - val_loss: 0.5553 - val_precision: 0.7126 - val_recall: 0.6585\n",
      "Epoch 8/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7205 - auc: 0.7885 - loss: 0.5572 - precision: 0.7222 - recall: 0.7022 - val_accuracy: 0.7228 - val_auc: 0.7922 - val_loss: 0.5534 - val_precision: 0.7076 - val_recall: 0.6752\n",
      "Epoch 9/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7192 - auc: 0.7909 - loss: 0.5546 - precision: 0.7185 - recall: 0.7058 - val_accuracy: 0.7239 - val_auc: 0.7934 - val_loss: 0.5517 - val_precision: 0.7133 - val_recall: 0.6663\n",
      "Epoch 10/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7205 - auc: 0.7922 - loss: 0.5531 - precision: 0.7192 - recall: 0.7085 - val_accuracy: 0.7244 - val_auc: 0.7939 - val_loss: 0.5517 - val_precision: 0.7106 - val_recall: 0.6741\n",
      "Epoch 11/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7245 - auc: 0.7938 - loss: 0.5518 - precision: 0.7247 - recall: 0.7094 - val_accuracy: 0.7213 - val_auc: 0.7950 - val_loss: 0.5500 - val_precision: 0.7042 - val_recall: 0.6775\n",
      "Epoch 12/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7252 - auc: 0.7952 - loss: 0.5496 - precision: 0.7236 - recall: 0.7141 - val_accuracy: 0.7213 - val_auc: 0.7958 - val_loss: 0.5494 - val_precision: 0.7009 - val_recall: 0.6853\n",
      "Epoch 13/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7255 - auc: 0.7961 - loss: 0.5486 - precision: 0.7251 - recall: 0.7119 - val_accuracy: 0.7223 - val_auc: 0.7961 - val_loss: 0.5488 - val_precision: 0.7011 - val_recall: 0.6886\n",
      "Epoch 14/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7219 - auc: 0.7973 - loss: 0.5474 - precision: 0.7184 - recall: 0.7150 - val_accuracy: 0.7208 - val_auc: 0.7962 - val_loss: 0.5479 - val_precision: 0.7015 - val_recall: 0.6819\n",
      "Epoch 15/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7236 - auc: 0.7973 - loss: 0.5470 - precision: 0.7225 - recall: 0.7112 - val_accuracy: 0.7203 - val_auc: 0.7971 - val_loss: 0.5473 - val_precision: 0.6993 - val_recall: 0.6853\n",
      "Epoch 16/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7238 - auc: 0.7976 - loss: 0.5467 - precision: 0.7223 - recall: 0.7125 - val_accuracy: 0.7234 - val_auc: 0.7972 - val_loss: 0.5498 - val_precision: 0.6918 - val_recall: 0.7165\n",
      "Epoch 17/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7240 - auc: 0.7988 - loss: 0.5452 - precision: 0.7207 - recall: 0.7168 - val_accuracy: 0.7198 - val_auc: 0.7974 - val_loss: 0.5475 - val_precision: 0.6945 - val_recall: 0.6953\n",
      "Epoch 18/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7245 - auc: 0.7992 - loss: 0.5450 - precision: 0.7208 - recall: 0.7181 - val_accuracy: 0.7213 - val_auc: 0.7970 - val_loss: 0.5484 - val_precision: 0.6943 - val_recall: 0.7020\n",
      "Epoch 19/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7234 - auc: 0.7996 - loss: 0.5445 - precision: 0.7203 - recall: 0.7158 - val_accuracy: 0.7223 - val_auc: 0.7969 - val_loss: 0.5483 - val_precision: 0.6954 - val_recall: 0.7031\n",
      "Epoch 20/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7251 - auc: 0.8007 - loss: 0.5433 - precision: 0.7217 - recall: 0.7181 - val_accuracy: 0.7208 - val_auc: 0.7970 - val_loss: 0.5488 - val_precision: 0.6922 - val_recall: 0.7054\n",
      "Epoch 21/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7268 - auc: 0.8019 - loss: 0.5421 - precision: 0.7244 - recall: 0.7178 - val_accuracy: 0.7208 - val_auc: 0.7974 - val_loss: 0.5482 - val_precision: 0.6926 - val_recall: 0.7042\n",
      "Epoch 22/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7279 - auc: 0.8018 - loss: 0.5420 - precision: 0.7259 - recall: 0.7181 - val_accuracy: 0.7218 - val_auc: 0.7976 - val_loss: 0.5477 - val_precision: 0.6950 - val_recall: 0.7020\n",
      "Epoch 23/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7270 - auc: 0.8021 - loss: 0.5418 - precision: 0.7266 - recall: 0.7138 - val_accuracy: 0.7213 - val_auc: 0.7977 - val_loss: 0.5479 - val_precision: 0.6913 - val_recall: 0.7098\n",
      "Epoch 24/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7265 - auc: 0.8030 - loss: 0.5405 - precision: 0.7238 - recall: 0.7183 - val_accuracy: 0.7228 - val_auc: 0.7975 - val_loss: 0.5476 - val_precision: 0.6944 - val_recall: 0.7076\n",
      "Epoch 25/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7299 - auc: 0.8033 - loss: 0.5404 - precision: 0.7272 - recall: 0.7217 - val_accuracy: 0.7269 - val_auc: 0.7980 - val_loss: 0.5470 - val_precision: 0.7042 - val_recall: 0.6987\n",
      "Epoch 26/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7304 - auc: 0.8033 - loss: 0.5406 - precision: 0.7281 - recall: 0.7214 - val_accuracy: 0.7264 - val_auc: 0.7978 - val_loss: 0.5473 - val_precision: 0.7002 - val_recall: 0.7065\n",
      "Epoch 27/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7299 - auc: 0.8038 - loss: 0.5398 - precision: 0.7267 - recall: 0.7229 - val_accuracy: 0.7269 - val_auc: 0.7980 - val_loss: 0.5466 - val_precision: 0.7028 - val_recall: 0.7020\n",
      "Epoch 28/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7319 - auc: 0.8039 - loss: 0.5397 - precision: 0.7320 - recall: 0.7181 - val_accuracy: 0.7269 - val_auc: 0.7981 - val_loss: 0.5474 - val_precision: 0.6975 - val_recall: 0.7154\n",
      "Epoch 29/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7316 - auc: 0.8045 - loss: 0.5390 - precision: 0.7288 - recall: 0.7239 - val_accuracy: 0.7295 - val_auc: 0.7984 - val_loss: 0.5462 - val_precision: 0.7054 - val_recall: 0.7054\n",
      "Epoch 30/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7331 - auc: 0.8051 - loss: 0.5383 - precision: 0.7321 - recall: 0.7212 - val_accuracy: 0.7295 - val_auc: 0.7985 - val_loss: 0.5457 - val_precision: 0.7081 - val_recall: 0.6987\n",
      "Epoch 31/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7319 - auc: 0.8049 - loss: 0.5385 - precision: 0.7317 - recall: 0.7190 - val_accuracy: 0.7305 - val_auc: 0.7984 - val_loss: 0.5454 - val_precision: 0.7102 - val_recall: 0.6975\n",
      "Epoch 32/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7303 - auc: 0.8051 - loss: 0.5382 - precision: 0.7283 - recall: 0.7211 - val_accuracy: 0.7295 - val_auc: 0.7987 - val_loss: 0.5449 - val_precision: 0.7100 - val_recall: 0.6942\n",
      "Epoch 33/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7305 - auc: 0.8051 - loss: 0.5382 - precision: 0.7305 - recall: 0.7169 - val_accuracy: 0.7321 - val_auc: 0.7985 - val_loss: 0.5456 - val_precision: 0.7103 - val_recall: 0.7031\n",
      "Epoch 34/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.7306 - auc: 0.8059 - loss: 0.5372 - precision: 0.7270 - recall: 0.7248 - val_accuracy: 0.7316 - val_auc: 0.7985 - val_loss: 0.5454 - val_precision: 0.7095 - val_recall: 0.7031\n",
      "Epoch 35/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7289 - auc: 0.8058 - loss: 0.5374 - precision: 0.7272 - recall: 0.7188 - val_accuracy: 0.7300 - val_auc: 0.7987 - val_loss: 0.5459 - val_precision: 0.7043 - val_recall: 0.7098\n",
      "Epoch 36/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7300 - auc: 0.8060 - loss: 0.5371 - precision: 0.7282 - recall: 0.7205 - val_accuracy: 0.7310 - val_auc: 0.7987 - val_loss: 0.5455 - val_precision: 0.7096 - val_recall: 0.7009\n",
      "Epoch 37/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7324 - auc: 0.8070 - loss: 0.5361 - precision: 0.7289 - recall: 0.7262 - val_accuracy: 0.7290 - val_auc: 0.7988 - val_loss: 0.5450 - val_precision: 0.7078 - val_recall: 0.6975\n",
      "Epoch 38/200\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.7321 - auc: 0.8069 - loss: 0.5363 - precision: 0.7316 - recall: 0.7198 - val_accuracy: 0.7310 - val_auc: 0.7988 - val_loss: 0.5449 - val_precision: 0.7096 - val_recall: 0.7009\n",
      "Epoch 39/200\n",
      "\u001b[1m183/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7330 - auc: 0.8074 - loss: 0.5355 - precision: 0.7328 - recall: 0.7204"
     ]
    }
   ],
   "source": [
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, X_train_num, X_train_img, y_train, X_val_num, X_val_img, y_val, X_test_num, X_test_img, y_test, dataset_name, \"IGTD_model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, X_train_num, X_train_img, y_train, X_val_num, X_val_img, y_val, X_test_num, X_test_img, y_test, dataset_name, \"IGTD_Model2\")\n",
    "#model3_metrics = safe_compile_and_fit(model3, X_train_num, X_train_img, y_train, X_val_num, X_val_img, y_val, X_test_num, X_test_img, y_test, dataset_name, \"TINTO_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "#if model3_metrics:\n",
    "#    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(base_path):\n",
    "    best_accuracy = float('-inf')\n",
    "    best_folder = None\n",
    "\n",
    "    # Walk through all directories and files in the base path\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file == f'{dataset_name}_metrics.txt':\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Read metrics from the file\n",
    "                with open(file_path, 'r') as f:\n",
    "                    metrics = f.read()\n",
    "                \n",
    "                # Parse the metrics into a dictionary\n",
    "                metrics_dict = {}\n",
    "                for line in metrics.splitlines():\n",
    "                    key, value = line.split(': ')\n",
    "                    metrics_dict[key.strip()] = float(value.strip())\n",
    "                \n",
    "                # Check if the current folder has a better accuracy\n",
    "                if metrics_dict['test_accuracy'] > best_accuracy:\n",
    "                    best_accuracy = metrics_dict['test_accuracy']\n",
    "                    best_folder = root\n",
    "    \n",
    "    return best_folder, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_folder(old_folder_path):\n",
    "    # Extract the base name of the old folder\n",
    "    folder_name = os.path.basename(old_folder_path)\n",
    "    \n",
    "    # Create the new folder name by prepending \"best_\"\n",
    "    new_folder_name = f\"BEST_{folder_name}\"\n",
    "    \n",
    "    # Get the parent directory of the old folder\n",
    "    parent_dir = os.path.dirname(old_folder_path)\n",
    "    \n",
    "    # Create the full path for the new folder\n",
    "    new_folder_path = os.path.join(parent_dir, new_folder_name)\n",
    "    \n",
    "    # Rename the folder\n",
    "    os.rename(old_folder_path, new_folder_path)\n",
    "    \n",
    "    return new_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ViT+MLP_Binary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m best_folder, best_accuracy \u001b[38;5;241m=\u001b[39m find_best_model(base_path)\n\u001b[1;32m----> 4\u001b[0m best_folder \u001b[38;5;241m=\u001b[39m \u001b[43mrename_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest model folder: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[47], line 5\u001b[0m, in \u001b[0;36mrename_folder\u001b[1;34m(old_folder_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrename_folder\u001b[39m(old_folder_path):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Extract the base name of the old folder\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     folder_name \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_folder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Create the new folder name by prepending \"best_\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     new_folder_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBEST_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m<frozen ntpath>:244\u001b[0m, in \u001b[0;36mbasename\u001b[1;34m(p)\u001b[0m\n",
      "File \u001b[1;32m<frozen ntpath>:213\u001b[0m, in \u001b[0;36msplit\u001b[1;34m(p)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "base_path = f\"logs/{dataset_name}/ViT+MLP_Binary\"\n",
    "best_folder, best_accuracy = find_best_model(base_path)\n",
    "best_folder = rename_folder(best_folder)\n",
    "print(f\"Best model folder: {best_folder}\")\n",
    "print(f\"Best Accuracy: {best_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tinto-HNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
