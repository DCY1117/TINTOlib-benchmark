{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install einops pandas TINTOlib scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# einops library for tensor operations\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "# Custom TINTO library imports\n",
    "from TINTOlib.tinto import TINTO\n",
    "from TINTOlib.supertml import SuperTML\n",
    "from TINTOlib.igtd import IGTD\n",
    "from TINTOlib.refined import REFINED\n",
    "from TINTOlib.barGraph import BarGraph\n",
    "from TINTOlib.distanceMatrix import DistanceMatrix\n",
    "from TINTOlib.combination import Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 12.1\n",
      "cuDNN Version: 8907\n",
      "PyTorch Version: 2.3.1+cu121\n",
      "CUDA is available. PyTorch can use GPU.\n",
      "Current GPU: NVIDIA GeForce RTX 3080\n",
      "Is this tensor on GPU? True\n",
      "Is CUDA initialized? True\n",
      "Number of available GPUs: 1\n",
      "Current device index: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get CUDA version\n",
    "cuda_version = torch.version.cuda\n",
    "print(f\"CUDA Version: {cuda_version}\")\n",
    "\n",
    "# Get cuDNN version\n",
    "cudnn_version = torch.backends.cudnn.version()\n",
    "print(f\"cuDNN Version: {cudnn_version}\")\n",
    "\n",
    "# Get PyTorch version\n",
    "pytorch_version = torch.__version__\n",
    "print(f\"PyTorch Version: {pytorch_version}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use GPU.\")\n",
    "    \n",
    "    # Get the name of the current GPU\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Create a random tensor and move it to GPU to verify\n",
    "    x = torch.rand(5, 3)\n",
    "    print(f\"Is this tensor on GPU? {x.cuda().is_cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use CPU.\")\n",
    "\n",
    "# Additional check: is CUDA initialized?\n",
    "print(f\"Is CUDA initialized? {torch.cuda.is_initialized()}\")\n",
    "\n",
    "# Number of available GPUs\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Current device index\n",
    "print(f\"Current device index: {torch.cuda.current_device()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable to store dataset name\n",
    "dataset_name = 'boston'\n",
    "results_path = f'logs/{dataset_name}/CNN_Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"../Datasets_benchmark/Regression/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=32):\n",
    "\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(images_folder):\n",
    "        #Generate thet images\n",
    "        image_model.generateImages(df, images_folder)\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    img_paths = os.path.join(images_folder,problem_type+\".csv\")\n",
    "\n",
    "    print(img_paths)\n",
    "    \n",
    "    imgs = pd.read_csv(img_paths)\n",
    "\n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = images_folder + \"/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, df], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    df_x = combined_dataset.drop(df.columns[-1], axis=1).drop(\"values\", axis=1)\n",
    "    df_y = combined_dataset[\"values\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df_x, df_y, test_size=0.20, random_state=SEED)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.50, random_state=SEED)\n",
    "    # Numerical data\n",
    "    X_train_num = X_train.drop(\"images\", axis=1)\n",
    "    X_val_num = X_val.drop(\"images\", axis=1)\n",
    "    X_test_num = X_test.drop(\"images\", axis=1)\n",
    "\n",
    "    # Image data\n",
    "    X_train_img = np.array([cv2.imread(img) for img in X_train[\"images\"]])\n",
    "    X_val_img = np.array([cv2.imread(img) for img in X_val[\"images\"]])\n",
    "    X_test_img = np.array([cv2.imread(img) for img in X_test[\"images\"]])\n",
    "\n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale numerical data\n",
    "    X_train_num = pd.DataFrame(scaler.fit_transform(X_train_num), columns=X_train_num.columns)\n",
    "    X_val_num = pd.DataFrame(scaler.transform(X_val_num), columns=X_val_num.columns)\n",
    "    X_test_num = pd.DataFrame(scaler.transform(X_test_num), columns=X_test_num.columns)\n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "    height, width, channels = X_train_img[0].shape\n",
    "    imgs_shape = (channels, height, width)\n",
    "\n",
    "    print(\"Images shape: \", imgs_shape)\n",
    "    print(\"Attributes: \", attributes)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_num_tensor = torch.tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor = torch.tensor(X_val_num.values, dtype=torch.float32)\n",
    "    X_test_num_tensor = torch.tensor(X_test_num.values, dtype=torch.float32)\n",
    "    X_train_img_tensor = torch.tensor(X_train_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    X_val_img_tensor = torch.tensor(X_val_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    X_test_img_tensor = torch.tensor(X_test_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_img_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_img_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_img_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes, imgs_shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Model1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=8, kernel_size=3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Calculate the size of the flattened output\n",
    "        conv_output_size = 8 * (input_shape[1]-2) * (input_shape[2]-2)\n",
    "\n",
    "        self.fc1 = nn.Linear(conv_output_size, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, imgs_shape):\n",
    "        super(Model2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(imgs_shape[0], out_channels=16, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Calculate the size of the flattened output\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flat_size, 64)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        # Forward pass with dummy input to calculate flat size\n",
    "        dummy_input = torch.zeros(1, *imgs_shape)\n",
    "        x = self.pool(self.relu(self.conv1(dummy_input)))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        return x.size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, imgs_shape):\n",
    "        super(Model2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(imgs_shape[0], out_channels=16, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Calculate the size of the flattened output\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flat_size, 64)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        # Forward pass with dummy input to calculate flat size\n",
    "        dummy_input = torch.zeros(1, *imgs_shape)\n",
    "        x = self.pool(self.relu(self.conv1(dummy_input)))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        return x.size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self, imgs_shape):\n",
    "        super(Model3, self).__init__()\n",
    "        # The formula to calculate the padding is:\n",
    "        # padding = (kernel_size - 1) // 2\n",
    "\n",
    "\n",
    "        dropout = 0.1\n",
    "\n",
    "        # CNN branch 1\n",
    "        self.branch_1 = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # CNN branch 2\n",
    "        self.branch_2 = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], out_channels=16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Calculate the size of the flattened output\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        # Dense layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self.flat_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def _get_flat_size(self, cnn_input_shape):\n",
    "        # Forward pass with dummy input to calculate flat size\n",
    "        dummy_input = torch.zeros(1, *cnn_input_shape)\n",
    "        branch_1_out = self.branch_1(dummy_input)  # Changed tower_1 to branch_1\n",
    "        branch_2_out = self.branch_2(dummy_input)  # Changed tower_2 to branch_2\n",
    "        concat_out = torch.cat((branch_1_out, branch_2_out), dim=1)\n",
    "        return int(torch.prod(torch.tensor(concat_out.shape[1:])))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch_1_out = self.branch_1(x)\n",
    "        branch_2_out = self.branch_2(x)\n",
    "        merged_cnn = torch.cat((branch_1_out, branch_2_out), dim=1)\n",
    "        flattened = self.flatten(merged_cnn)\n",
    "        output = self.fc_layers(flattened)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([4, 1])\n",
      "Model forward pass successful!\n",
      "Output shape is correct!\n",
      "Backward pass successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming you have your Model3 class defined as in the previous message\n",
    "\n",
    "# Create an instance of the model\n",
    "imgs_shape = (3, 224, 224)  # Example input shape (channels, height, width)\n",
    "model = Model1(imgs_shape)\n",
    "\n",
    "# Create a random input tensor\n",
    "batch_size = 4\n",
    "input_tensor = torch.randn(batch_size, *imgs_shape)\n",
    "\n",
    "# Try to get an output from the model\n",
    "try:\n",
    "    output = model(input_tensor)\n",
    "    print(f\"Model output shape: {output.shape}\")\n",
    "    print(\"Model forward pass successful!\")\n",
    "    \n",
    "    # Check if the output shape is as expected (batch_size, 1)\n",
    "    assert output.shape == (batch_size, 1), f\"Expected output shape (4, 1), but got {output.shape}\"\n",
    "    print(\"Output shape is correct!\")\n",
    "    \n",
    "    # Check if the model can compute gradients\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    print(\"Backward pass successful!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPILE AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=32, epochs=200, lr=1e-3, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = 20\n",
    "    best_model = None\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mse': [], 'val_mse': [], 'train_rmse': [], 'val_rmse': [], 'learning_rate': [], 'epoch_time': []}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "        for img_data, targets in train_loader:\n",
    "            img_data, targets = img_data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(outputs.cpu().detach().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for img_data, targets in train_loader:\n",
    "                img_data, targets = img_data.to(device), targets.to(device)\n",
    "                outputs = model(img_data)\n",
    "                val_loss += loss_fn(outputs, targets).item()\n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "\n",
    "        # Calculate MSE and RMSE\n",
    "        train_mse = mean_squared_error(train_targets, train_predictions)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        val_mse = mean_squared_error(val_targets, val_predictions)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mse'].append(train_mse)\n",
    "        history['val_mse'].append(val_mse)\n",
    "        history['train_rmse'].append(train_rmse)\n",
    "        history['val_rmse'].append(val_rmse)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, '\n",
    "            f'Train MSE: {train_mse:.4f}, Val MSE: {val_mse:.4f}, '\n",
    "            f'Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}, '\n",
    "            f'Time: {epoch_time:.2f}s')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Calculate and save metrics\n",
    "    train_metrics = calculate_metrics(model, train_loader, device)\n",
    "    val_metrics = calculate_metrics(model, val_loader, device)\n",
    "    test_metrics = calculate_metrics(model, test_loader, device)\n",
    "\n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'train_mse': train_metrics['mse'],\n",
    "        'train_mae': train_metrics['mae'],\n",
    "        'train_rmse': train_metrics['rmse'],\n",
    "        'train_r2': train_metrics['r2'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_mse': val_metrics['mse'],\n",
    "        'val_mae': val_metrics['mae'],\n",
    "        'val_rmse': val_metrics['rmse'],\n",
    "        'val_r2': val_metrics['r2'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_mse': test_metrics['mse'],\n",
    "        'test_mae': test_metrics['mae'],\n",
    "        'test_rmse': test_metrics['rmse'],\n",
    "        'test_r2': test_metrics['r2'],\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "\n",
    "    # Save figures\n",
    "    os.makedirs(f\"models/{dataset_name}/{model_name}\", exist_ok=True)\n",
    "    plot_metric(history['train_loss'], history['val_loss'], 'Loss', dataset_name, model_name)\n",
    "    plot_metric(history['train_mse'], history['val_mse'], 'MSE', dataset_name, model_name)\n",
    "    plot_metric(history['train_rmse'], history['val_rmse'], 'RMSE', dataset_name, model_name)\n",
    "    plot_learning_rate(history['learning_rate'], dataset_name, model_name)\n",
    "\n",
    "    # Save metrics to a file\n",
    "    os.makedirs(f'{results_path}/{model_name}', exist_ok=True)\n",
    "    with open(f'{results_path}/{model_name}/{dataset_name}_metrics.txt', 'w') as f:\n",
    "        for key, value in metrics.items():\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def calculate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_data, targets in data_loader:\n",
    "            img_data, targets = img_data.to(device), targets.to(device)\n",
    "            outputs = model(img_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "\n",
    "    mse = mean_squared_error(all_targets, all_predictions)\n",
    "    mae = mean_absolute_error(all_targets, all_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(all_targets, all_predictions)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }    \n",
    "\n",
    "def plot_metric(train_metric, val_metric, metric_name, dataset_name, model_name):\n",
    "    plt.figure()\n",
    "    plt.plot(train_metric, label=f'Train {metric_name}')\n",
    "    plt.plot(val_metric, label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'{metric_name} vs. Epoch')\n",
    "    plt.savefig(f\"models/{dataset_name}/{model_name}/{metric_name.lower()}_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_learning_rate(learning_rates, dataset_name, model_name):\n",
    "    plt.figure()\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate vs. Epoch')\n",
    "    plt.yscale('log')  # Use log scale for better visualization\n",
    "    plt.savefig(f\"models/{dataset_name}/{model_name}/learning_rate_plot.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=32, epochs=200, lr=1e-3, device='cuda'):\n",
    "    try:\n",
    "        if model is None:\n",
    "            print(f\"Model {model_name} is None\")\n",
    "            return None\n",
    "        else:\n",
    "            # Compile and fit the model\n",
    "            metrics = compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, epochs=epochs, lr=lr, device=device)\n",
    "            return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compile and fit {model_name}: {str(e)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Clear CUDA cache and force garbage collection\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_create_model(model_class, imgs_shape):\n",
    "    try:\n",
    "        model = model_class(imgs_shape)\n",
    "        \n",
    "        # Test the model with a sample input\n",
    "        sample_input = torch.randn(1, *imgs_shape)\n",
    "        output = model(sample_input)\n",
    "        \n",
    "        print(f\"Successfully created and tested {model_class.__name__}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or testing {model_class.__name__}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, random_seed=SEED)\n",
    "#image_model = REFINED(problem= problem_type,hcIterations=5)\n",
    "#image_model = IGTD(problem= problem_type)\n",
    "#image_model = BarGraph(problem= problem_type)\n",
    "#image_model = DistanceMatrix(problem= problem_type)\n",
    "#image_model = Combination(problem= problem_type)\n",
    "#image_model = SuperTML(problem= problem_type)\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_IGTD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default the seed is 1 for all experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 1: TINTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, random_seed=SEED)\n",
    "name = f\"TINTO_blur\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images are already generated\n",
      "../HyNNImages/Regression/boston/images_boston_TINTO_blur\\regression.csv\n",
      "Images shape:  (3, 20, 20)\n",
      "Attributes:  13\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 20, 20)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Train loader:\n",
      "Batch 1:\n",
      "  Numerical data shape: torch.Size([32, 13])\n",
      "  Image data shape: torch.Size([32, 3, 20, 20])\n",
      "  Labels shape: torch.Size([32, 1])\n",
      "  Sample numerical data: tensor([0.0021, 0.2316, 0.1979, 0.0000, 0.0947])\n",
      "  Sample image data: tensor([252., 252., 249., 247., 250.])\n",
      "  Sample label: tensor([20.5000])\n",
      "Batch 2:\n",
      "  Numerical data shape: torch.Size([32, 13])\n",
      "  Image data shape: torch.Size([32, 3, 20, 20])\n",
      "  Labels shape: torch.Size([32, 1])\n",
      "  Sample numerical data: tensor([0.0747, 0.0000, 0.6466, 0.0000, 0.6749])\n",
      "  Sample image data: tensor([255., 255., 255., 253., 254.])\n",
      "  Sample label: tensor([19.5000])\n",
      "Batch 3:\n",
      "  Numerical data shape: torch.Size([32, 13])\n",
      "  Image data shape: torch.Size([32, 3, 20, 20])\n",
      "  Labels shape: torch.Size([32, 1])\n",
      "  Sample numerical data: tensor([1.5477e-04, 1.0000e+00, 8.1378e-02, 0.0000e+00, 6.3992e-02])\n",
      "  Sample image data: tensor([240., 240., 229., 239., 245.])\n",
      "  Sample label: tensor([50.])\n",
      "\n",
      "Testing Validation loader:\n",
      "Batch 1:\n",
      "  Numerical data shape: torch.Size([32, 13])\n",
      "  Image data shape: torch.Size([32, 3, 20, 20])\n",
      "  Labels shape: torch.Size([32, 1])\n",
      "  Sample numerical data: tensor([0.1599, 0.0000, 0.6466, 0.0000, 0.6337])\n",
      "  Sample image data: tensor([255., 255., 255., 255., 254.])\n",
      "  Sample label: tensor([7.2000])\n",
      "Batch 2:\n",
      "  Numerical data shape: torch.Size([19, 13])\n",
      "  Image data shape: torch.Size([19, 3, 20, 20])\n",
      "  Labels shape: torch.Size([19, 1])\n",
      "  Sample numerical data: tensor([0.0659, 0.0000, 0.6466, 0.0000, 0.6337])\n",
      "  Sample image data: tensor([255., 255., 255., 255., 255.])\n",
      "  Sample label: tensor([12.5000])\n",
      "\n",
      "Testing Test loader:\n",
      "Batch 1:\n",
      "  Numerical data shape: torch.Size([32, 13])\n",
      "  Image data shape: torch.Size([32, 3, 20, 20])\n",
      "  Labels shape: torch.Size([32, 1])\n",
      "  Sample numerical data: tensor([0.0029, 0.0000, 0.3460, 0.0000, 0.3272])\n",
      "  Sample image data: tensor([255., 255., 255., 253., 254.])\n",
      "  Sample label: tensor([19.4000])\n",
      "Batch 2:\n",
      "  Numerical data shape: torch.Size([19, 13])\n",
      "  Image data shape: torch.Size([19, 3, 20, 20])\n",
      "  Labels shape: torch.Size([19, 1])\n",
      "  Sample numerical data: tensor([0.0182, 0.0000, 0.7856, 0.0000, 0.4918])\n",
      "  Sample image data: tensor([255., 255., 255., 255., 255.])\n",
      "  Sample label: tensor([14.4000])\n",
      "\n",
      "Testing DataLoaders:\n"
     ]
    }
   ],
   "source": [
    "# Add this function within load_and_preprocess_data\n",
    "def test_dataloaders(train_loader, val_loader, test_loader):\n",
    "    loaders = {'Train': train_loader, 'Validation': val_loader, 'Test': test_loader}\n",
    "    \n",
    "    for name, loader in loaders.items():\n",
    "        print(f\"\\nTesting {name} loader:\")\n",
    "        for i, (num_data, img_data, labels) in enumerate(loader):\n",
    "            print(f\"Batch {i+1}:\")\n",
    "            print(f\"  Numerical data shape: {num_data.shape}\")\n",
    "            print(f\"  Image data shape: {img_data.shape}\")\n",
    "            print(f\"  Labels shape: {labels.shape}\")\n",
    "            \n",
    "            # Print a sample of numerical data, image data, and labels\n",
    "            print(f\"  Sample numerical data: {num_data[0][:5]}\")  # First 5 values of first item\n",
    "            print(f\"  Sample image data: {img_data[0, 0, 0, :5]}\")  # First 5 pixels of first channel of first image\n",
    "            print(f\"  Sample label: {labels[0]}\")\n",
    "            \n",
    "            if i == 2:  # Only check the first 3 batches\n",
    "                break\n",
    "\n",
    "    # Test the loaders\n",
    "    print(\"\\nTesting DataLoaders:\")\n",
    "\n",
    "test_dataloaders(train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and tested Model1\n",
      "Successfully created and tested Model2\n",
      "Successfully created and tested Model3\n",
      "Epoch 1/200, Train Loss: 602.8452, Val Loss: 3782.6731, Train MSE: 603.2300, Val MSE: 581.9751, Train RMSE: 24.5607, Val RMSE: 24.1242, Time: 0.14s\n",
      "Epoch 2/200, Train Loss: 583.8660, Val Loss: 3773.0845, Train MSE: 581.7198, Val MSE: 581.4077, Train RMSE: 24.1189, Val RMSE: 24.1124, Time: 0.14s\n",
      "Epoch 3/200, Train Loss: 580.9226, Val Loss: 3760.0336, Train MSE: 581.1493, Val MSE: 580.8339, Train RMSE: 24.1070, Val RMSE: 24.1005, Time: 0.12s\n",
      "Epoch 4/200, Train Loss: 579.6756, Val Loss: 3793.8100, Train MSE: 580.5779, Val MSE: 580.2585, Train RMSE: 24.0952, Val RMSE: 24.0886, Time: 0.08s\n",
      "Epoch 5/200, Train Loss: 581.1775, Val Loss: 3754.9919, Train MSE: 580.0013, Val MSE: 579.6854, Train RMSE: 24.0832, Val RMSE: 24.0767, Time: 0.07s\n",
      "Epoch 6/200, Train Loss: 583.3792, Val Loss: 3722.7636, Train MSE: 579.4250, Val MSE: 579.1127, Train RMSE: 24.0712, Val RMSE: 24.0648, Time: 0.10s\n",
      "Epoch 7/200, Train Loss: 578.3527, Val Loss: 3759.1322, Train MSE: 578.8575, Val MSE: 578.5326, Train RMSE: 24.0595, Val RMSE: 24.0527, Time: 0.12s\n",
      "Epoch 8/200, Train Loss: 576.8532, Val Loss: 3761.5269, Train MSE: 578.2771, Val MSE: 577.9612, Train RMSE: 24.0474, Val RMSE: 24.0408, Time: 0.08s\n",
      "Epoch 9/200, Train Loss: 572.6348, Val Loss: 3755.5285, Train MSE: 577.7056, Val MSE: 577.3885, Train RMSE: 24.0355, Val RMSE: 24.0289, Time: 0.08s\n",
      "Epoch 10/200, Train Loss: 579.1735, Val Loss: 3770.7265, Train MSE: 577.1361, Val MSE: 576.8159, Train RMSE: 24.0237, Val RMSE: 24.0170, Time: 0.06s\n",
      "Epoch 11/200, Train Loss: 577.8192, Val Loss: 3745.9496, Train MSE: 576.5638, Val MSE: 576.2423, Train RMSE: 24.0117, Val RMSE: 24.0050, Time: 0.06s\n",
      "Epoch 12/200, Train Loss: 573.7107, Val Loss: 3740.4871, Train MSE: 575.9870, Val MSE: 575.6734, Train RMSE: 23.9997, Val RMSE: 23.9932, Time: 0.08s\n",
      "Epoch 13/200, Train Loss: 577.7774, Val Loss: 3722.9529, Train MSE: 575.4167, Val MSE: 575.1036, Train RMSE: 23.9878, Val RMSE: 23.9813, Time: 0.06s\n",
      "Epoch 14/200, Train Loss: 581.5519, Val Loss: 3745.5091, Train MSE: 574.8436, Val MSE: 574.5347, Train RMSE: 23.9759, Val RMSE: 23.9695, Time: 0.07s\n",
      "Epoch 15/200, Train Loss: 581.7540, Val Loss: 3768.0567, Train MSE: 574.2725, Val MSE: 573.9625, Train RMSE: 23.9640, Val RMSE: 23.9575, Time: 0.07s\n",
      "Epoch 16/200, Train Loss: 573.2074, Val Loss: 3747.7287, Train MSE: 573.7034, Val MSE: 573.3854, Train RMSE: 23.9521, Val RMSE: 23.9455, Time: 0.08s\n",
      "Epoch 17/200, Train Loss: 568.6220, Val Loss: 3745.0770, Train MSE: 573.1295, Val MSE: 572.8143, Train RMSE: 23.9401, Val RMSE: 23.9335, Time: 0.06s\n",
      "Epoch 18/200, Train Loss: 574.3645, Val Loss: 3699.4593, Train MSE: 572.7885, Val MSE: 572.7578, Train RMSE: 23.9330, Val RMSE: 23.9324, Time: 0.06s\n",
      "Epoch 19/200, Train Loss: 569.3496, Val Loss: 3713.4616, Train MSE: 572.7321, Val MSE: 572.7006, Train RMSE: 23.9318, Val RMSE: 23.9312, Time: 0.06s\n",
      "Epoch 20/200, Train Loss: 571.8159, Val Loss: 3741.2171, Train MSE: 572.6753, Val MSE: 572.6434, Train RMSE: 23.9306, Val RMSE: 23.9300, Time: 0.06s\n",
      "Epoch 21/200, Train Loss: 572.8575, Val Loss: 3723.4908, Train MSE: 572.6179, Val MSE: 572.5866, Train RMSE: 23.9294, Val RMSE: 23.9288, Time: 0.07s\n",
      "Epoch 22/200, Train Loss: 575.4358, Val Loss: 3725.3780, Train MSE: 572.5609, Val MSE: 572.5297, Train RMSE: 23.9282, Val RMSE: 23.9276, Time: 0.07s\n",
      "Epoch 23/200, Train Loss: 573.6176, Val Loss: 3720.7684, Train MSE: 572.5042, Val MSE: 572.4725, Train RMSE: 23.9271, Val RMSE: 23.9264, Time: 0.06s\n",
      "Epoch 24/200, Train Loss: 571.6368, Val Loss: 3711.3059, Train MSE: 572.4471, Val MSE: 572.4150, Train RMSE: 23.9259, Val RMSE: 23.9252, Time: 0.07s\n",
      "Epoch 25/200, Train Loss: 576.2786, Val Loss: 3706.2229, Train MSE: 572.3896, Val MSE: 572.3582, Train RMSE: 23.9247, Val RMSE: 23.9240, Time: 0.08s\n",
      "Epoch 26/200, Train Loss: 572.0620, Val Loss: 3705.6194, Train MSE: 572.3326, Val MSE: 572.3010, Train RMSE: 23.9235, Val RMSE: 23.9228, Time: 0.07s\n",
      "Epoch 27/200, Train Loss: 569.3440, Val Loss: 3713.2592, Train MSE: 572.2759, Val MSE: 572.2438, Train RMSE: 23.9223, Val RMSE: 23.9216, Time: 0.07s\n",
      "Epoch 28/200, Train Loss: 568.2975, Val Loss: 3737.9097, Train MSE: 572.2185, Val MSE: 572.1870, Train RMSE: 23.9211, Val RMSE: 23.9204, Time: 0.06s\n",
      "Epoch 29/200, Train Loss: 567.2308, Val Loss: 3750.9481, Train MSE: 572.1616, Val MSE: 572.1303, Train RMSE: 23.9199, Val RMSE: 23.9192, Time: 0.07s\n",
      "Epoch 30/200, Train Loss: 577.2262, Val Loss: 3740.2679, Train MSE: 572.1278, Val MSE: 572.1247, Train RMSE: 23.9192, Val RMSE: 23.9191, Time: 0.07s\n",
      "Epoch 31/200, Train Loss: 572.5167, Val Loss: 3686.0249, Train MSE: 572.1221, Val MSE: 572.1190, Train RMSE: 23.9191, Val RMSE: 23.9190, Time: 0.07s\n",
      "Epoch 32/200, Train Loss: 572.3563, Val Loss: 3706.0946, Train MSE: 572.1164, Val MSE: 572.1132, Train RMSE: 23.9190, Val RMSE: 23.9189, Time: 0.06s\n",
      "Epoch 33/200, Train Loss: 573.1071, Val Loss: 3726.9754, Train MSE: 572.1107, Val MSE: 572.1076, Train RMSE: 23.9188, Val RMSE: 23.9188, Time: 0.06s\n",
      "Epoch 34/200, Train Loss: 577.5918, Val Loss: 3701.0462, Train MSE: 572.1050, Val MSE: 572.1019, Train RMSE: 23.9187, Val RMSE: 23.9187, Time: 0.08s\n",
      "Epoch 35/200, Train Loss: 569.4041, Val Loss: 3711.0434, Train MSE: 572.0993, Val MSE: 572.0961, Train RMSE: 23.9186, Val RMSE: 23.9185, Time: 0.08s\n",
      "Epoch 36/200, Train Loss: 570.3392, Val Loss: 3733.1434, Train MSE: 572.0935, Val MSE: 572.0904, Train RMSE: 23.9185, Val RMSE: 23.9184, Time: 0.08s\n",
      "Epoch 37/200, Train Loss: 568.6782, Val Loss: 3705.0375, Train MSE: 572.0878, Val MSE: 572.0847, Train RMSE: 23.9184, Val RMSE: 23.9183, Time: 0.08s\n",
      "Epoch 38/200, Train Loss: 571.2290, Val Loss: 3715.2507, Train MSE: 572.0822, Val MSE: 572.0790, Train RMSE: 23.9182, Val RMSE: 23.9182, Time: 0.11s\n",
      "Epoch 39/200, Train Loss: 576.1264, Val Loss: 3712.7843, Train MSE: 572.0765, Val MSE: 572.0732, Train RMSE: 23.9181, Val RMSE: 23.9181, Time: 0.06s\n",
      "Epoch 40/200, Train Loss: 573.0734, Val Loss: 3717.2342, Train MSE: 572.0707, Val MSE: 572.0676, Train RMSE: 23.9180, Val RMSE: 23.9179, Time: 0.07s\n",
      "Epoch 41/200, Train Loss: 569.9549, Val Loss: 3722.9262, Train MSE: 572.0650, Val MSE: 572.0619, Train RMSE: 23.9179, Val RMSE: 23.9178, Time: 0.07s\n",
      "Epoch 42/200, Train Loss: 567.7516, Val Loss: 3728.3082, Train MSE: 572.0593, Val MSE: 572.0562, Train RMSE: 23.9178, Val RMSE: 23.9177, Time: 0.06s\n",
      "Epoch 43/200, Train Loss: 575.4365, Val Loss: 3748.7070, Train MSE: 572.0559, Val MSE: 572.0556, Train RMSE: 23.9177, Val RMSE: 23.9177, Time: 0.07s\n",
      "Epoch 44/200, Train Loss: 570.7495, Val Loss: 3719.5383, Train MSE: 572.0553, Val MSE: 572.0551, Train RMSE: 23.9177, Val RMSE: 23.9177, Time: 0.07s\n",
      "Epoch 45/200, Train Loss: 572.6401, Val Loss: 3720.2634, Train MSE: 572.0547, Val MSE: 572.0544, Train RMSE: 23.9177, Val RMSE: 23.9177, Time: 0.07s\n",
      "Epoch 46/200, Train Loss: 569.4261, Val Loss: 3742.9629, Train MSE: 572.0541, Val MSE: 572.0538, Train RMSE: 23.9177, Val RMSE: 23.9176, Time: 0.06s\n",
      "Epoch 47/200, Train Loss: 570.0936, Val Loss: 3684.0323, Train MSE: 572.0536, Val MSE: 572.0532, Train RMSE: 23.9176, Val RMSE: 23.9176, Time: 0.06s\n",
      "Epoch 48/200, Train Loss: 574.6899, Val Loss: 3725.3911, Train MSE: 572.0530, Val MSE: 572.0527, Train RMSE: 23.9176, Val RMSE: 23.9176, Time: 0.07s\n",
      "Epoch 49/200, Train Loss: 568.9656, Val Loss: 3688.7104, Train MSE: 572.0524, Val MSE: 572.0521, Train RMSE: 23.9176, Val RMSE: 23.9176, Time: 0.07s\n",
      "Epoch 50/200, Train Loss: 575.1925, Val Loss: 3711.3452, Train MSE: 572.0519, Val MSE: 572.0516, Train RMSE: 23.9176, Val RMSE: 23.9176, Time: 0.07s\n",
      "Epoch 51/200, Train Loss: 572.7060, Val Loss: 3692.3809, Train MSE: 572.0513, Val MSE: 572.0510, Train RMSE: 23.9176, Val RMSE: 23.9176, Time: 0.06s\n",
      "Epoch 52/200, Train Loss: 576.5359, Val Loss: 3738.3764, Train MSE: 572.0507, Val MSE: 572.0504, Train RMSE: 23.9176, Val RMSE: 23.9176, Time: 0.06s\n",
      "Epoch 53/200, Train Loss: 574.6872, Val Loss: 3703.8938, Train MSE: 572.0502, Val MSE: 572.0499, Train RMSE: 23.9176, Val RMSE: 23.9176, Time: 0.07s\n",
      "Epoch 54/200, Train Loss: 569.8568, Val Loss: 3701.4667, Train MSE: 572.0496, Val MSE: 572.0493, Train RMSE: 23.9176, Val RMSE: 23.9176, Time: 0.07s\n",
      "Epoch 55/200, Train Loss: 569.9032, Val Loss: 3721.9782, Train MSE: 572.0490, Val MSE: 572.0487, Train RMSE: 23.9175, Val RMSE: 23.9175, Time: 0.06s\n",
      "Epoch 56/200, Train Loss: 568.9250, Val Loss: 3757.7536, Train MSE: 572.0484, Val MSE: 572.0481, Train RMSE: 23.9175, Val RMSE: 23.9175, Time: 0.07s\n",
      "Epoch 57/200, Train Loss: 573.6950, Val Loss: 3683.6555, Train MSE: 572.0479, Val MSE: 572.0475, Train RMSE: 23.9175, Val RMSE: 23.9175, Time: 0.09s\n",
      "Epoch 58/200, Train Loss: 573.8930, Val Loss: 3706.4258, Train MSE: 572.0472, Val MSE: 572.0469, Train RMSE: 23.9175, Val RMSE: 23.9175, Time: 0.06s\n",
      "Epoch 59/200, Train Loss: 570.2954, Val Loss: 3717.5752, Train MSE: 572.0467, Val MSE: 572.0464, Train RMSE: 23.9175, Val RMSE: 23.9175, Time: 0.06s\n",
      "Epoch 60/200, Train Loss: 573.1021, Val Loss: 3699.6886, Train MSE: 572.0462, Val MSE: 572.0458, Train RMSE: 23.9175, Val RMSE: 23.9175, Time: 0.06s\n",
      "Epoch 61/200, Train Loss: 567.6232, Val Loss: 3703.1321, Train MSE: 572.0455, Val MSE: 572.0452, Train RMSE: 23.9175, Val RMSE: 23.9175, Time: 0.07s\n",
      "Epoch 62/200, Train Loss: 570.4424, Val Loss: 3729.5419, Train MSE: 572.0450, Val MSE: 572.0447, Train RMSE: 23.9175, Val RMSE: 23.9175, Time: 0.06s\n",
      "Epoch 63/200, Train Loss: 574.9828, Val Loss: 3725.9012, Train MSE: 572.0445, Val MSE: 572.0441, Train RMSE: 23.9175, Val RMSE: 23.9174, Time: 0.06s\n",
      "Epoch 64/200, Train Loss: 575.6061, Val Loss: 3725.1595, Train MSE: 572.0439, Val MSE: 572.0436, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.06s\n",
      "Epoch 65/200, Train Loss: 568.1590, Val Loss: 3702.5221, Train MSE: 572.0432, Val MSE: 572.0430, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.08s\n",
      "Epoch 66/200, Train Loss: 569.2755, Val Loss: 3715.9750, Train MSE: 572.0427, Val MSE: 572.0424, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.10s\n",
      "Epoch 67/200, Train Loss: 566.0305, Val Loss: 3700.1440, Train MSE: 572.0422, Val MSE: 572.0419, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.10s\n",
      "Epoch 68/200, Train Loss: 570.7611, Val Loss: 3723.8400, Train MSE: 572.0416, Val MSE: 572.0413, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.09s\n",
      "Epoch 69/200, Train Loss: 571.8184, Val Loss: 3740.7026, Train MSE: 572.0412, Val MSE: 572.0412, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.08s\n",
      "Epoch 70/200, Train Loss: 573.3350, Val Loss: 3707.6491, Train MSE: 572.0412, Val MSE: 572.0412, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.09s\n",
      "Epoch 71/200, Train Loss: 575.3130, Val Loss: 3716.2714, Train MSE: 572.0411, Val MSE: 572.0411, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.09s\n",
      "Epoch 72/200, Train Loss: 575.6394, Val Loss: 3711.6351, Train MSE: 572.0411, Val MSE: 572.0410, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.09s\n",
      "Epoch 73/200, Train Loss: 572.9203, Val Loss: 3741.0163, Train MSE: 572.0410, Val MSE: 572.0410, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.11s\n",
      "Epoch 74/200, Train Loss: 570.2834, Val Loss: 3711.3514, Train MSE: 572.0410, Val MSE: 572.0410, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.11s\n",
      "Epoch 75/200, Train Loss: 571.0331, Val Loss: 3693.4648, Train MSE: 572.0410, Val MSE: 572.0409, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.10s\n",
      "Epoch 76/200, Train Loss: 570.7442, Val Loss: 3736.6700, Train MSE: 572.0408, Val MSE: 572.0408, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.08s\n",
      "Epoch 77/200, Train Loss: 573.7768, Val Loss: 3761.7099, Train MSE: 572.0408, Val MSE: 572.0408, Train RMSE: 23.9174, Val RMSE: 23.9174, Time: 0.11s\n",
      "Early stopping\n",
      "Epoch 1/200, Train Loss: 567.0729, Val Loss: 783.3033, Train MSE: 581.0822, Val MSE: 119.8752, Train RMSE: 24.1056, Val RMSE: 10.9488, Time: 0.22s\n",
      "Epoch 2/200, Train Loss: 97.1054, Val Loss: 562.0273, Train MSE: 98.0765, Val MSE: 85.0600, Train RMSE: 9.9034, Val RMSE: 9.2228, Time: 0.12s\n",
      "Epoch 3/200, Train Loss: 91.4980, Val Loss: 527.8811, Train MSE: 91.7900, Val MSE: 81.9438, Train RMSE: 9.5807, Val RMSE: 9.0523, Time: 0.09s\n",
      "Epoch 4/200, Train Loss: 81.5136, Val Loss: 475.8757, Train MSE: 81.8675, Val MSE: 74.5252, Train RMSE: 9.0481, Val RMSE: 8.6328, Time: 0.08s\n",
      "Epoch 5/200, Train Loss: 79.0310, Val Loss: 581.3102, Train MSE: 78.1688, Val MSE: 88.8274, Train RMSE: 8.8413, Val RMSE: 9.4248, Time: 0.08s\n",
      "Epoch 6/200, Train Loss: 84.1201, Val Loss: 552.9536, Train MSE: 83.9090, Val MSE: 83.1555, Train RMSE: 9.1602, Val RMSE: 9.1190, Time: 0.07s\n",
      "Epoch 7/200, Train Loss: 72.1435, Val Loss: 394.3773, Train MSE: 73.2064, Val MSE: 60.7533, Train RMSE: 8.5561, Val RMSE: 7.7944, Time: 0.08s\n",
      "Epoch 8/200, Train Loss: 66.0603, Val Loss: 374.3590, Train MSE: 66.0988, Val MSE: 58.3048, Train RMSE: 8.1301, Val RMSE: 7.6358, Time: 0.07s\n",
      "Epoch 9/200, Train Loss: 61.7685, Val Loss: 359.0413, Train MSE: 61.5257, Val MSE: 54.8270, Train RMSE: 7.8438, Val RMSE: 7.4045, Time: 0.10s\n",
      "Epoch 10/200, Train Loss: 59.1321, Val Loss: 334.8231, Train MSE: 57.9937, Val MSE: 52.3240, Train RMSE: 7.6154, Val RMSE: 7.2335, Time: 0.12s\n",
      "Epoch 11/200, Train Loss: 56.7695, Val Loss: 395.8219, Train MSE: 57.0544, Val MSE: 61.4673, Train RMSE: 7.5534, Val RMSE: 7.8401, Time: 0.08s\n",
      "Epoch 12/200, Train Loss: 61.3094, Val Loss: 320.8568, Train MSE: 60.1128, Val MSE: 48.6974, Train RMSE: 7.7532, Val RMSE: 6.9784, Time: 0.08s\n",
      "Epoch 13/200, Train Loss: 51.8183, Val Loss: 368.5411, Train MSE: 52.8270, Val MSE: 57.2455, Train RMSE: 7.2682, Val RMSE: 7.5661, Time: 0.09s\n",
      "Epoch 14/200, Train Loss: 56.4343, Val Loss: 334.1308, Train MSE: 56.3400, Val MSE: 50.9861, Train RMSE: 7.5060, Val RMSE: 7.1405, Time: 0.08s\n",
      "Epoch 15/200, Train Loss: 58.9333, Val Loss: 411.1347, Train MSE: 59.1119, Val MSE: 63.7286, Train RMSE: 7.6884, Val RMSE: 7.9830, Time: 0.09s\n",
      "Epoch 16/200, Train Loss: 53.4054, Val Loss: 260.9590, Train MSE: 54.1124, Val MSE: 40.8115, Train RMSE: 7.3561, Val RMSE: 6.3884, Time: 0.09s\n",
      "Epoch 17/200, Train Loss: 46.4672, Val Loss: 261.1142, Train MSE: 43.8370, Val MSE: 40.2934, Train RMSE: 6.6209, Val RMSE: 6.3477, Time: 0.09s\n",
      "Epoch 18/200, Train Loss: 45.8834, Val Loss: 293.1351, Train MSE: 45.3913, Val MSE: 45.5838, Train RMSE: 6.7373, Val RMSE: 6.7516, Time: 0.07s\n",
      "Epoch 19/200, Train Loss: 44.0629, Val Loss: 269.2110, Train MSE: 43.9487, Val MSE: 40.6932, Train RMSE: 6.6294, Val RMSE: 6.3791, Time: 0.08s\n",
      "Epoch 20/200, Train Loss: 41.6998, Val Loss: 263.1150, Train MSE: 41.0580, Val MSE: 40.3734, Train RMSE: 6.4077, Val RMSE: 6.3540, Time: 0.08s\n",
      "Epoch 21/200, Train Loss: 38.6776, Val Loss: 209.7397, Train MSE: 38.0835, Val MSE: 32.9037, Train RMSE: 6.1712, Val RMSE: 5.7362, Time: 0.08s\n",
      "Epoch 22/200, Train Loss: 34.9181, Val Loss: 189.7124, Train MSE: 35.1625, Val MSE: 29.4699, Train RMSE: 5.9298, Val RMSE: 5.4286, Time: 0.08s\n",
      "Epoch 23/200, Train Loss: 35.0524, Val Loss: 214.0604, Train MSE: 35.3675, Val MSE: 33.2559, Train RMSE: 5.9471, Val RMSE: 5.7668, Time: 0.08s\n",
      "Epoch 24/200, Train Loss: 35.0833, Val Loss: 165.3196, Train MSE: 34.3616, Val MSE: 25.9324, Train RMSE: 5.8619, Val RMSE: 5.0924, Time: 0.12s\n",
      "Epoch 25/200, Train Loss: 35.4357, Val Loss: 176.7311, Train MSE: 35.5471, Val MSE: 26.4835, Train RMSE: 5.9621, Val RMSE: 5.1462, Time: 0.11s\n",
      "Epoch 26/200, Train Loss: 30.5588, Val Loss: 189.0153, Train MSE: 31.1631, Val MSE: 28.7535, Train RMSE: 5.5824, Val RMSE: 5.3622, Time: 0.08s\n",
      "Epoch 27/200, Train Loss: 30.8035, Val Loss: 159.4405, Train MSE: 31.0181, Val MSE: 24.5698, Train RMSE: 5.5694, Val RMSE: 4.9568, Time: 0.09s\n",
      "Epoch 28/200, Train Loss: 29.3149, Val Loss: 148.4201, Train MSE: 29.4878, Val MSE: 23.1485, Train RMSE: 5.4303, Val RMSE: 4.8113, Time: 0.08s\n",
      "Epoch 29/200, Train Loss: 32.2661, Val Loss: 208.5340, Train MSE: 32.5621, Val MSE: 30.6765, Train RMSE: 5.7063, Val RMSE: 5.5386, Time: 0.08s\n",
      "Epoch 30/200, Train Loss: 29.8575, Val Loss: 151.8487, Train MSE: 30.4924, Val MSE: 23.6959, Train RMSE: 5.5220, Val RMSE: 4.8678, Time: 0.07s\n",
      "Epoch 31/200, Train Loss: 29.4785, Val Loss: 145.2066, Train MSE: 29.8563, Val MSE: 22.5514, Train RMSE: 5.4641, Val RMSE: 4.7488, Time: 0.08s\n",
      "Epoch 32/200, Train Loss: 30.8166, Val Loss: 220.5011, Train MSE: 30.5097, Val MSE: 33.3732, Train RMSE: 5.5236, Val RMSE: 5.7770, Time: 0.08s\n",
      "Epoch 33/200, Train Loss: 35.4719, Val Loss: 140.7738, Train MSE: 35.9118, Val MSE: 21.6171, Train RMSE: 5.9926, Val RMSE: 4.6494, Time: 0.07s\n",
      "Epoch 34/200, Train Loss: 29.9155, Val Loss: 145.2086, Train MSE: 30.2897, Val MSE: 22.5161, Train RMSE: 5.5036, Val RMSE: 4.7451, Time: 0.09s\n",
      "Epoch 35/200, Train Loss: 25.8779, Val Loss: 133.2644, Train MSE: 26.2614, Val MSE: 20.7480, Train RMSE: 5.1246, Val RMSE: 4.5550, Time: 0.08s\n",
      "Epoch 36/200, Train Loss: 24.4641, Val Loss: 129.5748, Train MSE: 24.8806, Val MSE: 20.1905, Train RMSE: 4.9880, Val RMSE: 4.4934, Time: 0.08s\n",
      "Epoch 37/200, Train Loss: 26.7625, Val Loss: 124.9790, Train MSE: 26.6155, Val MSE: 19.5663, Train RMSE: 5.1590, Val RMSE: 4.4234, Time: 0.07s\n",
      "Epoch 38/200, Train Loss: 26.1721, Val Loss: 121.2198, Train MSE: 26.5060, Val MSE: 18.6845, Train RMSE: 5.1484, Val RMSE: 4.3226, Time: 0.07s\n",
      "Epoch 39/200, Train Loss: 25.0048, Val Loss: 123.4816, Train MSE: 23.8002, Val MSE: 18.9649, Train RMSE: 4.8786, Val RMSE: 4.3549, Time: 0.10s\n",
      "Epoch 40/200, Train Loss: 29.0149, Val Loss: 156.7054, Train MSE: 27.8941, Val MSE: 24.4218, Train RMSE: 5.2815, Val RMSE: 4.9418, Time: 0.09s\n",
      "Epoch 41/200, Train Loss: 26.7217, Val Loss: 128.0455, Train MSE: 26.1683, Val MSE: 19.9031, Train RMSE: 5.1155, Val RMSE: 4.4613, Time: 0.08s\n",
      "Epoch 42/200, Train Loss: 23.4816, Val Loss: 123.9629, Train MSE: 23.7708, Val MSE: 18.5965, Train RMSE: 4.8755, Val RMSE: 4.3124, Time: 0.10s\n",
      "Epoch 43/200, Train Loss: 21.1208, Val Loss: 119.3860, Train MSE: 21.4749, Val MSE: 18.0657, Train RMSE: 4.6341, Val RMSE: 4.2504, Time: 0.11s\n",
      "Epoch 44/200, Train Loss: 25.5138, Val Loss: 126.6943, Train MSE: 25.4765, Val MSE: 19.2578, Train RMSE: 5.0474, Val RMSE: 4.3884, Time: 0.07s\n",
      "Epoch 45/200, Train Loss: 23.1231, Val Loss: 111.8754, Train MSE: 23.5744, Val MSE: 17.4392, Train RMSE: 4.8554, Val RMSE: 4.1760, Time: 0.09s\n",
      "Epoch 46/200, Train Loss: 25.6296, Val Loss: 123.3522, Train MSE: 25.0668, Val MSE: 18.8227, Train RMSE: 5.0067, Val RMSE: 4.3385, Time: 0.08s\n",
      "Epoch 47/200, Train Loss: 21.7088, Val Loss: 111.8187, Train MSE: 21.8459, Val MSE: 16.8904, Train RMSE: 4.6740, Val RMSE: 4.1098, Time: 0.13s\n",
      "Epoch 48/200, Train Loss: 23.5308, Val Loss: 186.7935, Train MSE: 23.9518, Val MSE: 29.1363, Train RMSE: 4.8941, Val RMSE: 5.3978, Time: 0.11s\n",
      "Epoch 49/200, Train Loss: 25.6432, Val Loss: 120.4493, Train MSE: 25.9431, Val MSE: 18.4237, Train RMSE: 5.0934, Val RMSE: 4.2923, Time: 0.10s\n",
      "Epoch 50/200, Train Loss: 24.3076, Val Loss: 108.5509, Train MSE: 24.4932, Val MSE: 17.0617, Train RMSE: 4.9491, Val RMSE: 4.1306, Time: 0.13s\n",
      "Epoch 51/200, Train Loss: 22.2196, Val Loss: 115.4174, Train MSE: 22.4796, Val MSE: 17.0694, Train RMSE: 4.7413, Val RMSE: 4.1315, Time: 0.10s\n",
      "Epoch 52/200, Train Loss: 23.4811, Val Loss: 112.1328, Train MSE: 23.8664, Val MSE: 17.0852, Train RMSE: 4.8853, Val RMSE: 4.1334, Time: 0.10s\n",
      "Epoch 53/200, Train Loss: 21.5381, Val Loss: 115.3039, Train MSE: 21.5842, Val MSE: 18.1722, Train RMSE: 4.6459, Val RMSE: 4.2629, Time: 0.09s\n",
      "Epoch 54/200, Train Loss: 24.8768, Val Loss: 150.6656, Train MSE: 25.2190, Val MSE: 22.5135, Train RMSE: 5.0219, Val RMSE: 4.7448, Time: 0.09s\n",
      "Epoch 55/200, Train Loss: 21.7029, Val Loss: 118.1910, Train MSE: 21.8790, Val MSE: 18.3347, Train RMSE: 4.6775, Val RMSE: 4.2819, Time: 0.08s\n",
      "Epoch 56/200, Train Loss: 20.9802, Val Loss: 109.4045, Train MSE: 21.2151, Val MSE: 16.6251, Train RMSE: 4.6060, Val RMSE: 4.0774, Time: 0.10s\n",
      "Epoch 57/200, Train Loss: 24.5730, Val Loss: 185.3090, Train MSE: 24.8347, Val MSE: 28.9847, Train RMSE: 4.9834, Val RMSE: 5.3837, Time: 0.10s\n",
      "Epoch 58/200, Train Loss: 23.2087, Val Loss: 109.4089, Train MSE: 23.1532, Val MSE: 16.7286, Train RMSE: 4.8118, Val RMSE: 4.0901, Time: 0.08s\n",
      "Epoch 59/200, Train Loss: 22.0222, Val Loss: 115.3060, Train MSE: 22.2490, Val MSE: 18.1024, Train RMSE: 4.7169, Val RMSE: 4.2547, Time: 0.09s\n",
      "Epoch 60/200, Train Loss: 23.7467, Val Loss: 121.1503, Train MSE: 23.6853, Val MSE: 19.0204, Train RMSE: 4.8668, Val RMSE: 4.3612, Time: 0.09s\n",
      "Epoch 61/200, Train Loss: 24.4990, Val Loss: 116.3432, Train MSE: 23.5883, Val MSE: 17.2514, Train RMSE: 4.8568, Val RMSE: 4.1535, Time: 0.08s\n",
      "Epoch 62/200, Train Loss: 20.8406, Val Loss: 100.0904, Train MSE: 21.0740, Val MSE: 15.6189, Train RMSE: 4.5906, Val RMSE: 3.9521, Time: 0.08s\n",
      "Epoch 63/200, Train Loss: 20.9865, Val Loss: 100.7614, Train MSE: 21.0452, Val MSE: 15.6917, Train RMSE: 4.5875, Val RMSE: 3.9613, Time: 0.09s\n",
      "Epoch 64/200, Train Loss: 19.8167, Val Loss: 98.2757, Train MSE: 19.9945, Val MSE: 15.3863, Train RMSE: 4.4715, Val RMSE: 3.9225, Time: 0.10s\n",
      "Epoch 65/200, Train Loss: 20.5424, Val Loss: 98.2938, Train MSE: 20.4182, Val MSE: 15.4373, Train RMSE: 4.5186, Val RMSE: 3.9290, Time: 0.09s\n",
      "Epoch 66/200, Train Loss: 19.1091, Val Loss: 95.0753, Train MSE: 19.4013, Val MSE: 14.9513, Train RMSE: 4.4047, Val RMSE: 3.8667, Time: 0.09s\n",
      "Epoch 67/200, Train Loss: 17.8608, Val Loss: 96.3671, Train MSE: 18.2008, Val MSE: 15.0073, Train RMSE: 4.2662, Val RMSE: 3.8739, Time: 0.09s\n",
      "Epoch 68/200, Train Loss: 19.9676, Val Loss: 95.6527, Train MSE: 19.9963, Val MSE: 15.0258, Train RMSE: 4.4717, Val RMSE: 3.8763, Time: 0.11s\n",
      "Epoch 69/200, Train Loss: 18.5216, Val Loss: 95.5066, Train MSE: 18.7731, Val MSE: 14.7896, Train RMSE: 4.3328, Val RMSE: 3.8457, Time: 0.12s\n",
      "Epoch 70/200, Train Loss: 18.0676, Val Loss: 94.4832, Train MSE: 18.3955, Val MSE: 14.8344, Train RMSE: 4.2890, Val RMSE: 3.8515, Time: 0.10s\n",
      "Epoch 71/200, Train Loss: 18.1038, Val Loss: 94.4481, Train MSE: 17.9639, Val MSE: 14.7537, Train RMSE: 4.2384, Val RMSE: 3.8411, Time: 0.08s\n",
      "Epoch 72/200, Train Loss: 20.0713, Val Loss: 95.4352, Train MSE: 20.1818, Val MSE: 14.6926, Train RMSE: 4.4924, Val RMSE: 3.8331, Time: 0.09s\n",
      "Epoch 73/200, Train Loss: 17.5522, Val Loss: 93.2906, Train MSE: 17.7383, Val MSE: 14.6444, Train RMSE: 4.2117, Val RMSE: 3.8268, Time: 0.09s\n",
      "Epoch 74/200, Train Loss: 20.0267, Val Loss: 100.8693, Train MSE: 19.6283, Val MSE: 14.7708, Train RMSE: 4.4304, Val RMSE: 3.8433, Time: 0.09s\n",
      "Epoch 75/200, Train Loss: 17.8829, Val Loss: 92.9672, Train MSE: 17.9970, Val MSE: 14.5855, Train RMSE: 4.2423, Val RMSE: 3.8191, Time: 0.09s\n",
      "Epoch 76/200, Train Loss: 17.6570, Val Loss: 101.7326, Train MSE: 17.9799, Val MSE: 14.5414, Train RMSE: 4.2403, Val RMSE: 3.8133, Time: 0.10s\n",
      "Epoch 77/200, Train Loss: 18.2841, Val Loss: 94.3587, Train MSE: 18.3942, Val MSE: 14.5332, Train RMSE: 4.2888, Val RMSE: 3.8122, Time: 0.08s\n",
      "Epoch 78/200, Train Loss: 18.9903, Val Loss: 94.2889, Train MSE: 19.1429, Val MSE: 14.5219, Train RMSE: 4.3753, Val RMSE: 3.8108, Time: 0.10s\n",
      "Epoch 79/200, Train Loss: 18.9334, Val Loss: 94.6998, Train MSE: 19.2631, Val MSE: 14.4548, Train RMSE: 4.3890, Val RMSE: 3.8019, Time: 0.10s\n",
      "Epoch 80/200, Train Loss: 18.6516, Val Loss: 93.0436, Train MSE: 18.8465, Val MSE: 14.4325, Train RMSE: 4.3413, Val RMSE: 3.7990, Time: 0.08s\n",
      "Epoch 81/200, Train Loss: 19.2913, Val Loss: 93.1815, Train MSE: 19.4231, Val MSE: 14.5188, Train RMSE: 4.4072, Val RMSE: 3.8104, Time: 0.10s\n",
      "Epoch 82/200, Train Loss: 17.3203, Val Loss: 91.7971, Train MSE: 17.5989, Val MSE: 14.3920, Train RMSE: 4.1951, Val RMSE: 3.7937, Time: 0.11s\n",
      "Epoch 83/200, Train Loss: 18.6408, Val Loss: 92.6111, Train MSE: 18.8409, Val MSE: 14.4793, Train RMSE: 4.3406, Val RMSE: 3.8052, Time: 0.10s\n",
      "Epoch 84/200, Train Loss: 18.6123, Val Loss: 98.0901, Train MSE: 18.9058, Val MSE: 15.1437, Train RMSE: 4.3481, Val RMSE: 3.8915, Time: 0.08s\n",
      "Epoch 85/200, Train Loss: 18.9040, Val Loss: 97.2945, Train MSE: 19.0190, Val MSE: 14.2814, Train RMSE: 4.3611, Val RMSE: 3.7791, Time: 0.10s\n",
      "Epoch 86/200, Train Loss: 19.2169, Val Loss: 91.7417, Train MSE: 18.9421, Val MSE: 14.2634, Train RMSE: 4.3523, Val RMSE: 3.7767, Time: 0.10s\n",
      "Epoch 87/200, Train Loss: 18.2921, Val Loss: 90.9277, Train MSE: 18.5236, Val MSE: 14.2556, Train RMSE: 4.3039, Val RMSE: 3.7757, Time: 0.09s\n",
      "Epoch 88/200, Train Loss: 19.8314, Val Loss: 91.2082, Train MSE: 19.1996, Val MSE: 14.1744, Train RMSE: 4.3817, Val RMSE: 3.7649, Time: 0.09s\n",
      "Epoch 89/200, Train Loss: 16.5401, Val Loss: 98.2511, Train MSE: 16.5848, Val MSE: 14.2804, Train RMSE: 4.0724, Val RMSE: 3.7789, Time: 0.11s\n",
      "Epoch 90/200, Train Loss: 18.5530, Val Loss: 93.1801, Train MSE: 18.8739, Val MSE: 14.4686, Train RMSE: 4.3444, Val RMSE: 3.8038, Time: 0.13s\n",
      "Epoch 91/200, Train Loss: 18.7290, Val Loss: 90.4560, Train MSE: 18.3763, Val MSE: 14.1087, Train RMSE: 4.2868, Val RMSE: 3.7562, Time: 0.09s\n",
      "Epoch 92/200, Train Loss: 20.1395, Val Loss: 94.2350, Train MSE: 19.8760, Val MSE: 14.3852, Train RMSE: 4.4583, Val RMSE: 3.7928, Time: 0.10s\n",
      "Epoch 93/200, Train Loss: 19.6484, Val Loss: 90.3568, Train MSE: 19.9681, Val MSE: 14.0545, Train RMSE: 4.4686, Val RMSE: 3.7489, Time: 0.10s\n",
      "Epoch 94/200, Train Loss: 18.1438, Val Loss: 94.1488, Train MSE: 18.4809, Val MSE: 14.6538, Train RMSE: 4.2989, Val RMSE: 3.8280, Time: 0.10s\n",
      "Epoch 95/200, Train Loss: 20.3481, Val Loss: 98.4735, Train MSE: 19.8761, Val MSE: 14.1068, Train RMSE: 4.4583, Val RMSE: 3.7559, Time: 0.09s\n",
      "Epoch 96/200, Train Loss: 18.2389, Val Loss: 93.9996, Train MSE: 18.2596, Val MSE: 14.6002, Train RMSE: 4.2731, Val RMSE: 3.8210, Time: 0.09s\n",
      "Epoch 97/200, Train Loss: 18.9969, Val Loss: 89.8413, Train MSE: 18.4524, Val MSE: 14.1411, Train RMSE: 4.2956, Val RMSE: 3.7605, Time: 0.10s\n",
      "Epoch 98/200, Train Loss: 16.7190, Val Loss: 89.4187, Train MSE: 17.0053, Val MSE: 13.9960, Train RMSE: 4.1237, Val RMSE: 3.7411, Time: 0.10s\n",
      "Epoch 99/200, Train Loss: 16.3747, Val Loss: 93.1336, Train MSE: 16.1310, Val MSE: 13.9788, Train RMSE: 4.0163, Val RMSE: 3.7388, Time: 0.10s\n",
      "Epoch 100/200, Train Loss: 19.9660, Val Loss: 93.3266, Train MSE: 19.1902, Val MSE: 14.0936, Train RMSE: 4.3807, Val RMSE: 3.7541, Time: 0.12s\n",
      "Epoch 101/200, Train Loss: 18.2735, Val Loss: 89.8720, Train MSE: 18.5861, Val MSE: 14.1658, Train RMSE: 4.3112, Val RMSE: 3.7637, Time: 0.10s\n",
      "Epoch 102/200, Train Loss: 19.0945, Val Loss: 92.2777, Train MSE: 19.4002, Val MSE: 14.0486, Train RMSE: 4.4046, Val RMSE: 3.7481, Time: 0.09s\n",
      "Epoch 103/200, Train Loss: 18.9847, Val Loss: 91.5396, Train MSE: 19.3469, Val MSE: 14.1421, Train RMSE: 4.3985, Val RMSE: 3.7606, Time: 0.10s\n",
      "Epoch 104/200, Train Loss: 19.1162, Val Loss: 93.9321, Train MSE: 19.4393, Val MSE: 13.8815, Train RMSE: 4.4090, Val RMSE: 3.7258, Time: 0.09s\n",
      "Epoch 105/200, Train Loss: 18.2738, Val Loss: 90.8028, Train MSE: 18.1612, Val MSE: 14.0737, Train RMSE: 4.2616, Val RMSE: 3.7515, Time: 0.09s\n",
      "Epoch 106/200, Train Loss: 18.1502, Val Loss: 90.9087, Train MSE: 18.4133, Val MSE: 13.8815, Train RMSE: 4.2911, Val RMSE: 3.7258, Time: 0.13s\n",
      "Epoch 107/200, Train Loss: 15.6871, Val Loss: 95.0275, Train MSE: 15.7110, Val MSE: 14.0262, Train RMSE: 3.9637, Val RMSE: 3.7452, Time: 0.13s\n",
      "Epoch 108/200, Train Loss: 16.4998, Val Loss: 89.7372, Train MSE: 15.9232, Val MSE: 13.8918, Train RMSE: 3.9904, Val RMSE: 3.7272, Time: 0.11s\n",
      "Epoch 109/200, Train Loss: 19.6426, Val Loss: 89.0130, Train MSE: 20.0013, Val MSE: 13.9453, Train RMSE: 4.4723, Val RMSE: 3.7343, Time: 0.13s\n",
      "Epoch 110/200, Train Loss: 18.0450, Val Loss: 94.1059, Train MSE: 18.3277, Val MSE: 13.7547, Train RMSE: 4.2811, Val RMSE: 3.7087, Time: 0.09s\n",
      "Epoch 111/200, Train Loss: 19.4018, Val Loss: 89.1816, Train MSE: 17.9491, Val MSE: 13.9520, Train RMSE: 4.2366, Val RMSE: 3.7352, Time: 0.10s\n",
      "Epoch 112/200, Train Loss: 19.5207, Val Loss: 90.4532, Train MSE: 19.5137, Val MSE: 13.7940, Train RMSE: 4.4174, Val RMSE: 3.7140, Time: 0.10s\n",
      "Epoch 113/200, Train Loss: 17.7660, Val Loss: 89.5382, Train MSE: 17.9348, Val MSE: 13.9745, Train RMSE: 4.2350, Val RMSE: 3.7383, Time: 0.10s\n",
      "Epoch 114/200, Train Loss: 20.0687, Val Loss: 92.3895, Train MSE: 19.2194, Val MSE: 13.7256, Train RMSE: 4.3840, Val RMSE: 3.7048, Time: 0.10s\n",
      "Epoch 115/200, Train Loss: 15.9424, Val Loss: 89.2929, Train MSE: 16.2786, Val MSE: 13.8504, Train RMSE: 4.0347, Val RMSE: 3.7216, Time: 0.09s\n",
      "Epoch 116/200, Train Loss: 19.3071, Val Loss: 88.5583, Train MSE: 18.6464, Val MSE: 13.6231, Train RMSE: 4.3181, Val RMSE: 3.6909, Time: 0.10s\n",
      "Epoch 117/200, Train Loss: 18.5611, Val Loss: 89.1260, Train MSE: 18.7477, Val MSE: 13.5880, Train RMSE: 4.3299, Val RMSE: 3.6862, Time: 0.10s\n",
      "Epoch 118/200, Train Loss: 18.9405, Val Loss: 93.0446, Train MSE: 18.8775, Val MSE: 13.6472, Train RMSE: 4.3448, Val RMSE: 3.6942, Time: 0.09s\n",
      "Epoch 119/200, Train Loss: 18.6538, Val Loss: 86.8972, Train MSE: 18.0326, Val MSE: 13.5365, Train RMSE: 4.2465, Val RMSE: 3.6792, Time: 0.09s\n",
      "Epoch 120/200, Train Loss: 18.1921, Val Loss: 88.9473, Train MSE: 18.3065, Val MSE: 13.6159, Train RMSE: 4.2786, Val RMSE: 3.6900, Time: 0.10s\n",
      "Epoch 121/200, Train Loss: 18.1169, Val Loss: 90.6024, Train MSE: 17.9544, Val MSE: 13.5227, Train RMSE: 4.2373, Val RMSE: 3.6773, Time: 0.09s\n",
      "Epoch 122/200, Train Loss: 17.7430, Val Loss: 87.7754, Train MSE: 17.7869, Val MSE: 13.6206, Train RMSE: 4.2175, Val RMSE: 3.6906, Time: 0.09s\n",
      "Epoch 123/200, Train Loss: 16.3351, Val Loss: 90.6557, Train MSE: 16.4409, Val MSE: 13.5317, Train RMSE: 4.0547, Val RMSE: 3.6786, Time: 0.09s\n",
      "Epoch 124/200, Train Loss: 18.1917, Val Loss: 86.4093, Train MSE: 17.6414, Val MSE: 13.5728, Train RMSE: 4.2002, Val RMSE: 3.6841, Time: 0.10s\n",
      "Epoch 125/200, Train Loss: 17.5538, Val Loss: 86.3261, Train MSE: 17.6159, Val MSE: 13.4433, Train RMSE: 4.1971, Val RMSE: 3.6665, Time: 0.10s\n",
      "Epoch 126/200, Train Loss: 17.0094, Val Loss: 87.6669, Train MSE: 17.3004, Val MSE: 13.6941, Train RMSE: 4.1594, Val RMSE: 3.7006, Time: 0.09s\n",
      "Epoch 127/200, Train Loss: 18.1405, Val Loss: 84.8675, Train MSE: 18.0727, Val MSE: 13.3586, Train RMSE: 4.2512, Val RMSE: 3.6549, Time: 0.10s\n",
      "Epoch 128/200, Train Loss: 17.1403, Val Loss: 88.6439, Train MSE: 17.3905, Val MSE: 13.7292, Train RMSE: 4.1702, Val RMSE: 3.7053, Time: 0.10s\n",
      "Epoch 129/200, Train Loss: 16.0973, Val Loss: 85.0910, Train MSE: 16.0070, Val MSE: 13.3273, Train RMSE: 4.0009, Val RMSE: 3.6507, Time: 0.12s\n",
      "Epoch 130/200, Train Loss: 18.0790, Val Loss: 85.2560, Train MSE: 18.1583, Val MSE: 13.3284, Train RMSE: 4.2613, Val RMSE: 3.6508, Time: 0.11s\n",
      "Epoch 131/200, Train Loss: 19.3636, Val Loss: 85.1912, Train MSE: 19.2422, Val MSE: 13.2690, Train RMSE: 4.3866, Val RMSE: 3.6427, Time: 0.09s\n",
      "Epoch 132/200, Train Loss: 19.5803, Val Loss: 88.0353, Train MSE: 18.9249, Val MSE: 13.4605, Train RMSE: 4.3503, Val RMSE: 3.6689, Time: 0.09s\n",
      "Epoch 133/200, Train Loss: 16.5393, Val Loss: 86.2747, Train MSE: 16.2864, Val MSE: 13.4728, Train RMSE: 4.0356, Val RMSE: 3.6705, Time: 0.09s\n",
      "Epoch 134/200, Train Loss: 18.1968, Val Loss: 85.4220, Train MSE: 18.4742, Val MSE: 13.2800, Train RMSE: 4.2982, Val RMSE: 3.6442, Time: 0.08s\n",
      "Epoch 135/200, Train Loss: 15.6979, Val Loss: 87.1202, Train MSE: 15.7474, Val MSE: 13.2729, Train RMSE: 3.9683, Val RMSE: 3.6432, Time: 0.09s\n",
      "Epoch 136/200, Train Loss: 16.7195, Val Loss: 85.4785, Train MSE: 16.8822, Val MSE: 13.2836, Train RMSE: 4.1088, Val RMSE: 3.6447, Time: 0.10s\n",
      "Epoch 137/200, Train Loss: 17.7330, Val Loss: 85.2894, Train MSE: 17.3173, Val MSE: 13.3375, Train RMSE: 4.1614, Val RMSE: 3.6521, Time: 0.10s\n",
      "Epoch 138/200, Train Loss: 17.9249, Val Loss: 91.6346, Train MSE: 18.1395, Val MSE: 13.1509, Train RMSE: 4.2590, Val RMSE: 3.6264, Time: 0.09s\n",
      "Epoch 139/200, Train Loss: 17.3280, Val Loss: 87.4406, Train MSE: 17.4122, Val MSE: 13.1434, Train RMSE: 4.1728, Val RMSE: 3.6254, Time: 0.10s\n",
      "Epoch 140/200, Train Loss: 17.0847, Val Loss: 85.9503, Train MSE: 17.2720, Val MSE: 13.1845, Train RMSE: 4.1560, Val RMSE: 3.6310, Time: 0.10s\n",
      "Epoch 141/200, Train Loss: 15.9297, Val Loss: 84.0450, Train MSE: 16.1056, Val MSE: 13.1361, Train RMSE: 4.0132, Val RMSE: 3.6244, Time: 0.10s\n",
      "Epoch 142/200, Train Loss: 16.8220, Val Loss: 84.1691, Train MSE: 16.4695, Val MSE: 13.1495, Train RMSE: 4.0583, Val RMSE: 3.6262, Time: 0.11s\n",
      "Epoch 143/200, Train Loss: 18.4169, Val Loss: 83.7799, Train MSE: 18.4836, Val MSE: 13.1221, Train RMSE: 4.2993, Val RMSE: 3.6224, Time: 0.11s\n",
      "Epoch 144/200, Train Loss: 18.3734, Val Loss: 83.5473, Train MSE: 17.8714, Val MSE: 13.0769, Train RMSE: 4.2275, Val RMSE: 3.6162, Time: 0.08s\n",
      "Epoch 145/200, Train Loss: 16.1896, Val Loss: 88.0114, Train MSE: 16.2519, Val MSE: 13.0977, Train RMSE: 4.0314, Val RMSE: 3.6191, Time: 0.10s\n",
      "Epoch 146/200, Train Loss: 17.9264, Val Loss: 84.2120, Train MSE: 17.4351, Val MSE: 13.1054, Train RMSE: 4.1755, Val RMSE: 3.6201, Time: 0.10s\n",
      "Epoch 147/200, Train Loss: 16.5631, Val Loss: 86.0218, Train MSE: 16.6397, Val MSE: 13.0982, Train RMSE: 4.0792, Val RMSE: 3.6191, Time: 0.09s\n",
      "Epoch 148/200, Train Loss: 14.7724, Val Loss: 84.6188, Train MSE: 14.7498, Val MSE: 13.0910, Train RMSE: 3.8405, Val RMSE: 3.6181, Time: 0.09s\n",
      "Epoch 149/200, Train Loss: 16.5799, Val Loss: 84.3476, Train MSE: 16.7085, Val MSE: 13.0884, Train RMSE: 4.0876, Val RMSE: 3.6178, Time: 0.10s\n",
      "Epoch 150/200, Train Loss: 15.4209, Val Loss: 88.2037, Train MSE: 15.4733, Val MSE: 13.0681, Train RMSE: 3.9336, Val RMSE: 3.6150, Time: 0.08s\n",
      "Epoch 151/200, Train Loss: 16.5152, Val Loss: 84.6053, Train MSE: 16.7953, Val MSE: 13.0955, Train RMSE: 4.0982, Val RMSE: 3.6188, Time: 0.09s\n",
      "Epoch 152/200, Train Loss: 16.5209, Val Loss: 85.7459, Train MSE: 16.2969, Val MSE: 13.0583, Train RMSE: 4.0369, Val RMSE: 3.6136, Time: 0.11s\n",
      "Epoch 153/200, Train Loss: 18.0472, Val Loss: 84.6945, Train MSE: 17.2562, Val MSE: 13.0284, Train RMSE: 4.1541, Val RMSE: 3.6095, Time: 0.11s\n",
      "Epoch 154/200, Train Loss: 15.8359, Val Loss: 83.7961, Train MSE: 15.9093, Val MSE: 13.0399, Train RMSE: 3.9886, Val RMSE: 3.6111, Time: 0.10s\n",
      "Epoch 155/200, Train Loss: 16.2652, Val Loss: 87.4004, Train MSE: 16.1071, Val MSE: 13.0255, Train RMSE: 4.0134, Val RMSE: 3.6091, Time: 0.11s\n",
      "Epoch 156/200, Train Loss: 17.5333, Val Loss: 83.4275, Train MSE: 17.7752, Val MSE: 13.0278, Train RMSE: 4.2161, Val RMSE: 3.6094, Time: 0.11s\n",
      "Epoch 157/200, Train Loss: 16.0334, Val Loss: 83.7710, Train MSE: 16.0315, Val MSE: 13.0270, Train RMSE: 4.0039, Val RMSE: 3.6093, Time: 0.09s\n",
      "Epoch 158/200, Train Loss: 17.9761, Val Loss: 89.0196, Train MSE: 17.2560, Val MSE: 13.0268, Train RMSE: 4.1540, Val RMSE: 3.6093, Time: 0.09s\n",
      "Epoch 159/200, Train Loss: 19.3548, Val Loss: 83.2570, Train MSE: 18.7371, Val MSE: 13.0294, Train RMSE: 4.3286, Val RMSE: 3.6096, Time: 0.09s\n",
      "Epoch 160/200, Train Loss: 18.3162, Val Loss: 84.1159, Train MSE: 18.5340, Val MSE: 13.0284, Train RMSE: 4.3051, Val RMSE: 3.6095, Time: 0.09s\n",
      "Epoch 161/200, Train Loss: 15.5599, Val Loss: 83.0520, Train MSE: 15.6770, Val MSE: 13.0331, Train RMSE: 3.9594, Val RMSE: 3.6101, Time: 0.09s\n",
      "Epoch 162/200, Train Loss: 16.5508, Val Loss: 83.1999, Train MSE: 16.4916, Val MSE: 13.0371, Train RMSE: 4.0610, Val RMSE: 3.6107, Time: 0.11s\n",
      "Epoch 163/200, Train Loss: 16.4513, Val Loss: 85.6253, Train MSE: 16.5064, Val MSE: 13.0446, Train RMSE: 4.0628, Val RMSE: 3.6117, Time: 0.08s\n",
      "Epoch 164/200, Train Loss: 17.6135, Val Loss: 83.6601, Train MSE: 16.6696, Val MSE: 13.0475, Train RMSE: 4.0828, Val RMSE: 3.6121, Time: 0.09s\n",
      "Epoch 165/200, Train Loss: 17.3852, Val Loss: 83.5921, Train MSE: 16.7824, Val MSE: 13.0444, Train RMSE: 4.0966, Val RMSE: 3.6117, Time: 0.09s\n",
      "Epoch 166/200, Train Loss: 17.4673, Val Loss: 85.3819, Train MSE: 17.7145, Val MSE: 13.0406, Train RMSE: 4.2089, Val RMSE: 3.6112, Time: 0.09s\n",
      "Epoch 167/200, Train Loss: 16.6826, Val Loss: 83.3058, Train MSE: 16.8807, Val MSE: 13.0384, Train RMSE: 4.1086, Val RMSE: 3.6109, Time: 0.10s\n",
      "Epoch 168/200, Train Loss: 17.1680, Val Loss: 83.9435, Train MSE: 16.9807, Val MSE: 13.0371, Train RMSE: 4.1208, Val RMSE: 3.6107, Time: 0.10s\n",
      "Epoch 169/200, Train Loss: 17.2217, Val Loss: 85.8912, Train MSE: 17.0818, Val MSE: 13.0464, Train RMSE: 4.1330, Val RMSE: 3.6120, Time: 0.10s\n",
      "Epoch 170/200, Train Loss: 16.5944, Val Loss: 83.5305, Train MSE: 16.5946, Val MSE: 13.0448, Train RMSE: 4.0736, Val RMSE: 3.6118, Time: 0.10s\n",
      "Epoch 171/200, Train Loss: 16.7200, Val Loss: 85.3048, Train MSE: 16.7782, Val MSE: 13.0389, Train RMSE: 4.0961, Val RMSE: 3.6109, Time: 0.10s\n",
      "Epoch 172/200, Train Loss: 17.2301, Val Loss: 85.8718, Train MSE: 17.3074, Val MSE: 13.0385, Train RMSE: 4.1602, Val RMSE: 3.6109, Time: 0.14s\n",
      "Epoch 173/200, Train Loss: 17.2324, Val Loss: 83.5287, Train MSE: 17.4048, Val MSE: 13.0390, Train RMSE: 4.1719, Val RMSE: 3.6110, Time: 0.12s\n",
      "Epoch 174/200, Train Loss: 17.7745, Val Loss: 88.7032, Train MSE: 18.0004, Val MSE: 13.0384, Train RMSE: 4.2427, Val RMSE: 3.6109, Time: 0.10s\n",
      "Epoch 175/200, Train Loss: 17.1584, Val Loss: 84.1691, Train MSE: 17.1861, Val MSE: 13.0386, Train RMSE: 4.1456, Val RMSE: 3.6109, Time: 0.10s\n",
      "Epoch 176/200, Train Loss: 17.0693, Val Loss: 87.8701, Train MSE: 16.2989, Val MSE: 13.0384, Train RMSE: 4.0372, Val RMSE: 3.6109, Time: 0.08s\n",
      "Epoch 177/200, Train Loss: 17.7586, Val Loss: 88.6292, Train MSE: 18.1108, Val MSE: 13.0378, Train RMSE: 4.2557, Val RMSE: 3.6108, Time: 0.10s\n",
      "Epoch 178/200, Train Loss: 17.6134, Val Loss: 86.3092, Train MSE: 16.8440, Val MSE: 13.0375, Train RMSE: 4.1041, Val RMSE: 3.6108, Time: 0.09s\n",
      "Epoch 179/200, Train Loss: 17.2916, Val Loss: 83.6408, Train MSE: 17.2327, Val MSE: 13.0387, Train RMSE: 4.1512, Val RMSE: 3.6109, Time: 0.08s\n",
      "Epoch 180/200, Train Loss: 17.9413, Val Loss: 83.3019, Train MSE: 17.8956, Val MSE: 13.0382, Train RMSE: 4.2303, Val RMSE: 3.6108, Time: 0.11s\n",
      "Epoch 181/200, Train Loss: 16.8568, Val Loss: 83.6792, Train MSE: 17.1992, Val MSE: 13.0396, Train RMSE: 4.1472, Val RMSE: 3.6110, Time: 0.10s\n",
      "Early stopping\n",
      "Epoch 1/200, Train Loss: 581.0449, Val Loss: 3658.5165, Train MSE: 580.6298, Val MSE: 563.9568, Train RMSE: 24.0963, Val RMSE: 23.7478, Time: 0.45s\n",
      "Epoch 2/200, Train Loss: 551.1859, Val Loss: 3493.5088, Train MSE: 552.6192, Val MSE: 539.0005, Train RMSE: 23.5079, Val RMSE: 23.2164, Time: 0.26s\n",
      "Epoch 3/200, Train Loss: 528.1393, Val Loss: 3356.3046, Train MSE: 528.8348, Val MSE: 518.2938, Train RMSE: 22.9964, Val RMSE: 22.7661, Time: 0.25s\n",
      "Epoch 4/200, Train Loss: 513.0940, Val Loss: 3232.1287, Train MSE: 509.9211, Val MSE: 500.9678, Train RMSE: 22.5814, Val RMSE: 22.3823, Time: 0.30s\n",
      "Epoch 5/200, Train Loss: 491.7569, Val Loss: 3145.5803, Train MSE: 493.9821, Val MSE: 486.0337, Train RMSE: 22.2257, Val RMSE: 22.0462, Time: 0.22s\n",
      "Epoch 6/200, Train Loss: 481.7839, Val Loss: 3039.6337, Train MSE: 480.2525, Val MSE: 473.0147, Train RMSE: 21.9147, Val RMSE: 21.7489, Time: 0.22s\n",
      "Epoch 7/200, Train Loss: 467.4258, Val Loss: 2990.4204, Train MSE: 468.4358, Val MSE: 460.6357, Train RMSE: 21.6434, Val RMSE: 21.4624, Time: 0.23s\n",
      "Epoch 8/200, Train Loss: 453.5108, Val Loss: 2932.8949, Train MSE: 455.3604, Val MSE: 448.2477, Train RMSE: 21.3392, Val RMSE: 21.1719, Time: 0.28s\n",
      "Epoch 9/200, Train Loss: 441.2042, Val Loss: 2821.6393, Train MSE: 441.9169, Val MSE: 436.0000, Train RMSE: 21.0218, Val RMSE: 20.8806, Time: 0.24s\n",
      "Epoch 10/200, Train Loss: 424.3324, Val Loss: 2737.9896, Train MSE: 429.9764, Val MSE: 424.2429, Train RMSE: 20.7359, Val RMSE: 20.5972, Time: 0.25s\n",
      "Epoch 11/200, Train Loss: 421.5515, Val Loss: 2667.7502, Train MSE: 419.0044, Val MSE: 412.9621, Train RMSE: 20.4696, Val RMSE: 20.3215, Time: 0.23s\n",
      "Epoch 12/200, Train Loss: 413.3072, Val Loss: 2605.1178, Train MSE: 408.4857, Val MSE: 401.4966, Train RMSE: 20.2110, Val RMSE: 20.0374, Time: 0.25s\n",
      "Epoch 13/200, Train Loss: 398.6578, Val Loss: 2516.7053, Train MSE: 396.6247, Val MSE: 390.0696, Train RMSE: 19.9154, Val RMSE: 19.7502, Time: 0.23s\n",
      "Epoch 14/200, Train Loss: 384.9428, Val Loss: 2467.5361, Train MSE: 385.5997, Val MSE: 379.2477, Train RMSE: 19.6367, Val RMSE: 19.4743, Time: 0.24s\n",
      "Epoch 15/200, Train Loss: 370.2465, Val Loss: 2399.2389, Train MSE: 373.9887, Val MSE: 369.0237, Train RMSE: 19.3388, Val RMSE: 19.2100, Time: 0.24s\n",
      "Epoch 16/200, Train Loss: 361.2818, Val Loss: 2345.9234, Train MSE: 364.1257, Val MSE: 358.9799, Train RMSE: 19.0821, Val RMSE: 18.9468, Time: 0.25s\n",
      "Epoch 17/200, Train Loss: 351.2548, Val Loss: 2252.4850, Train MSE: 354.4318, Val MSE: 348.7615, Train RMSE: 18.8264, Val RMSE: 18.6752, Time: 0.22s\n",
      "Epoch 18/200, Train Loss: 344.6961, Val Loss: 2229.6735, Train MSE: 345.7785, Val MSE: 338.5043, Train RMSE: 18.5951, Val RMSE: 18.3985, Time: 0.24s\n",
      "Epoch 19/200, Train Loss: 336.9958, Val Loss: 2139.7720, Train MSE: 334.5289, Val MSE: 328.8262, Train RMSE: 18.2901, Val RMSE: 18.1336, Time: 0.24s\n",
      "Epoch 20/200, Train Loss: 326.7313, Val Loss: 2082.0947, Train MSE: 325.8251, Val MSE: 319.6646, Train RMSE: 18.0506, Val RMSE: 17.8792, Time: 0.23s\n",
      "Epoch 21/200, Train Loss: 319.6630, Val Loss: 2030.7940, Train MSE: 317.2970, Val MSE: 310.8013, Train RMSE: 17.8128, Val RMSE: 17.6296, Time: 0.25s\n",
      "Epoch 22/200, Train Loss: 304.0290, Val Loss: 1980.2471, Train MSE: 305.6776, Val MSE: 302.1248, Train RMSE: 17.4836, Val RMSE: 17.3817, Time: 0.24s\n",
      "Epoch 23/200, Train Loss: 299.1479, Val Loss: 1887.9002, Train MSE: 298.1036, Val MSE: 293.6669, Train RMSE: 17.2657, Val RMSE: 17.1367, Time: 0.23s\n",
      "Epoch 24/200, Train Loss: 290.1892, Val Loss: 1847.5665, Train MSE: 291.2384, Val MSE: 284.9862, Train RMSE: 17.0657, Val RMSE: 16.8815, Time: 0.27s\n",
      "Epoch 25/200, Train Loss: 278.9924, Val Loss: 1797.3602, Train MSE: 282.0110, Val MSE: 276.2057, Train RMSE: 16.7932, Val RMSE: 16.6194, Time: 0.24s\n",
      "Epoch 26/200, Train Loss: 282.2432, Val Loss: 1749.8852, Train MSE: 273.4697, Val MSE: 268.0238, Train RMSE: 16.5369, Val RMSE: 16.3714, Time: 0.22s\n",
      "Epoch 27/200, Train Loss: 265.1550, Val Loss: 1726.0459, Train MSE: 265.2386, Val MSE: 260.1213, Train RMSE: 16.2861, Val RMSE: 16.1283, Time: 0.24s\n",
      "Epoch 28/200, Train Loss: 257.8856, Val Loss: 1658.0652, Train MSE: 256.7915, Val MSE: 252.6851, Train RMSE: 16.0247, Val RMSE: 15.8961, Time: 0.25s\n",
      "Epoch 29/200, Train Loss: 249.7622, Val Loss: 1610.2207, Train MSE: 250.3191, Val MSE: 245.5630, Train RMSE: 15.8215, Val RMSE: 15.6705, Time: 0.23s\n",
      "Epoch 30/200, Train Loss: 241.6360, Val Loss: 1539.3788, Train MSE: 241.4265, Val MSE: 238.7163, Train RMSE: 15.5379, Val RMSE: 15.4504, Time: 0.25s\n",
      "Epoch 31/200, Train Loss: 234.3666, Val Loss: 1522.3028, Train MSE: 236.9176, Val MSE: 232.1856, Train RMSE: 15.3921, Val RMSE: 15.2376, Time: 0.26s\n",
      "Epoch 32/200, Train Loss: 231.1382, Val Loss: 1453.9969, Train MSE: 230.3927, Val MSE: 225.9118, Train RMSE: 15.1787, Val RMSE: 15.0304, Time: 0.23s\n",
      "Epoch 33/200, Train Loss: 226.2188, Val Loss: 1445.8601, Train MSE: 224.1974, Val MSE: 219.9046, Train RMSE: 14.9732, Val RMSE: 14.8292, Time: 0.22s\n",
      "Epoch 34/200, Train Loss: 216.5398, Val Loss: 1390.1164, Train MSE: 217.0780, Val MSE: 214.0669, Train RMSE: 14.7336, Val RMSE: 14.6310, Time: 0.31s\n",
      "Epoch 35/200, Train Loss: 213.2089, Val Loss: 1380.3923, Train MSE: 213.4796, Val MSE: 208.5116, Train RMSE: 14.6109, Val RMSE: 14.4399, Time: 0.24s\n",
      "Epoch 36/200, Train Loss: 206.0988, Val Loss: 1296.9539, Train MSE: 205.0621, Val MSE: 203.1994, Train RMSE: 14.3200, Val RMSE: 14.2548, Time: 0.23s\n",
      "Epoch 37/200, Train Loss: 197.6681, Val Loss: 1263.9531, Train MSE: 200.4456, Val MSE: 197.9211, Train RMSE: 14.1579, Val RMSE: 14.0684, Time: 0.29s\n",
      "Epoch 38/200, Train Loss: 197.1011, Val Loss: 1252.3066, Train MSE: 197.7321, Val MSE: 192.9953, Train RMSE: 14.0617, Val RMSE: 13.8923, Time: 0.24s\n",
      "Epoch 39/200, Train Loss: 188.2214, Val Loss: 1207.7407, Train MSE: 190.3830, Val MSE: 188.1988, Train RMSE: 13.7979, Val RMSE: 13.7186, Time: 0.28s\n",
      "Epoch 40/200, Train Loss: 189.2079, Val Loss: 1206.6687, Train MSE: 187.3434, Val MSE: 183.6415, Train RMSE: 13.6873, Val RMSE: 13.5514, Time: 0.23s\n",
      "Epoch 41/200, Train Loss: 184.3916, Val Loss: 1162.0785, Train MSE: 183.2849, Val MSE: 179.2026, Train RMSE: 13.5383, Val RMSE: 13.3867, Time: 0.24s\n",
      "Epoch 42/200, Train Loss: 175.6673, Val Loss: 1142.7805, Train MSE: 178.0331, Val MSE: 174.9429, Train RMSE: 13.3429, Val RMSE: 13.2266, Time: 0.24s\n",
      "Epoch 43/200, Train Loss: 171.8732, Val Loss: 1106.8010, Train MSE: 173.7982, Val MSE: 170.8438, Train RMSE: 13.1833, Val RMSE: 13.0707, Time: 0.23s\n",
      "Epoch 44/200, Train Loss: 166.6581, Val Loss: 1071.6855, Train MSE: 169.1642, Val MSE: 166.9346, Train RMSE: 13.0063, Val RMSE: 12.9203, Time: 0.24s\n",
      "Epoch 45/200, Train Loss: 163.8600, Val Loss: 1055.0436, Train MSE: 164.1239, Val MSE: 163.1958, Train RMSE: 12.8111, Val RMSE: 12.7748, Time: 0.26s\n",
      "Epoch 46/200, Train Loss: 164.8678, Val Loss: 1039.4449, Train MSE: 163.4456, Val MSE: 159.6164, Train RMSE: 12.7846, Val RMSE: 12.6339, Time: 0.25s\n",
      "Epoch 47/200, Train Loss: 160.0280, Val Loss: 1015.0198, Train MSE: 160.2120, Val MSE: 155.9830, Train RMSE: 12.6575, Val RMSE: 12.4893, Time: 0.24s\n",
      "Epoch 48/200, Train Loss: 156.3246, Val Loss: 982.3655, Train MSE: 157.1381, Val MSE: 152.6812, Train RMSE: 12.5355, Val RMSE: 12.3564, Time: 0.23s\n",
      "Epoch 49/200, Train Loss: 154.7784, Val Loss: 969.7778, Train MSE: 153.6032, Val MSE: 149.4789, Train RMSE: 12.3937, Val RMSE: 12.2262, Time: 0.28s\n",
      "Epoch 50/200, Train Loss: 148.9828, Val Loss: 965.3375, Train MSE: 148.5565, Val MSE: 146.3863, Train RMSE: 12.1884, Val RMSE: 12.0990, Time: 0.23s\n",
      "Epoch 51/200, Train Loss: 146.3965, Val Loss: 914.1105, Train MSE: 146.3436, Val MSE: 143.3916, Train RMSE: 12.0973, Val RMSE: 11.9746, Time: 0.25s\n",
      "Epoch 52/200, Train Loss: 141.8014, Val Loss: 908.0100, Train MSE: 142.6535, Val MSE: 140.6365, Train RMSE: 11.9438, Val RMSE: 11.8590, Time: 0.23s\n",
      "Epoch 53/200, Train Loss: 142.3486, Val Loss: 888.0108, Train MSE: 142.1472, Val MSE: 137.9019, Train RMSE: 11.9225, Val RMSE: 11.7432, Time: 0.21s\n",
      "Epoch 54/200, Train Loss: 139.5007, Val Loss: 868.3654, Train MSE: 138.3872, Val MSE: 135.2025, Train RMSE: 11.7638, Val RMSE: 11.6277, Time: 0.22s\n",
      "Epoch 55/200, Train Loss: 135.5990, Val Loss: 889.9198, Train MSE: 135.1442, Val MSE: 132.6364, Train RMSE: 11.6252, Val RMSE: 11.5168, Time: 0.23s\n",
      "Epoch 56/200, Train Loss: 131.6602, Val Loss: 851.7446, Train MSE: 132.1845, Val MSE: 130.2298, Train RMSE: 11.4972, Val RMSE: 11.4118, Time: 0.20s\n",
      "Epoch 57/200, Train Loss: 129.7605, Val Loss: 838.6318, Train MSE: 131.8703, Val MSE: 127.8839, Train RMSE: 11.4835, Val RMSE: 11.3086, Time: 0.21s\n",
      "Epoch 58/200, Train Loss: 128.2345, Val Loss: 805.7891, Train MSE: 127.9473, Val MSE: 125.7119, Train RMSE: 11.3114, Val RMSE: 11.2121, Time: 0.20s\n",
      "Epoch 59/200, Train Loss: 125.7625, Val Loss: 802.1656, Train MSE: 128.0409, Val MSE: 123.5471, Train RMSE: 11.3155, Val RMSE: 11.1152, Time: 0.21s\n",
      "Epoch 60/200, Train Loss: 123.6483, Val Loss: 795.3836, Train MSE: 122.0911, Val MSE: 121.6246, Train RMSE: 11.0495, Val RMSE: 11.0284, Time: 0.21s\n",
      "Epoch 61/200, Train Loss: 122.6133, Val Loss: 771.1060, Train MSE: 122.7773, Val MSE: 119.6623, Train RMSE: 11.0805, Val RMSE: 10.9390, Time: 0.21s\n",
      "Epoch 62/200, Train Loss: 118.7238, Val Loss: 769.7680, Train MSE: 120.0524, Val MSE: 117.8055, Train RMSE: 10.9568, Val RMSE: 10.8538, Time: 0.20s\n",
      "Epoch 63/200, Train Loss: 121.7167, Val Loss: 744.4866, Train MSE: 117.7550, Val MSE: 116.0593, Train RMSE: 10.8515, Val RMSE: 10.7731, Time: 0.21s\n",
      "Epoch 64/200, Train Loss: 116.3688, Val Loss: 742.3809, Train MSE: 115.6573, Val MSE: 114.2971, Train RMSE: 10.7544, Val RMSE: 10.6910, Time: 0.22s\n",
      "Epoch 65/200, Train Loss: 118.5109, Val Loss: 754.8715, Train MSE: 117.0391, Val MSE: 112.7023, Train RMSE: 10.8185, Val RMSE: 10.6161, Time: 0.22s\n",
      "Epoch 66/200, Train Loss: 111.7106, Val Loss: 731.0531, Train MSE: 113.0231, Val MSE: 111.1087, Train RMSE: 10.6312, Val RMSE: 10.5408, Time: 0.20s\n",
      "Epoch 67/200, Train Loss: 111.5935, Val Loss: 702.9188, Train MSE: 110.6154, Val MSE: 109.6744, Train RMSE: 10.5174, Val RMSE: 10.4726, Time: 0.21s\n",
      "Epoch 68/200, Train Loss: 108.8780, Val Loss: 695.4746, Train MSE: 109.8784, Val MSE: 108.2944, Train RMSE: 10.4823, Val RMSE: 10.4065, Time: 0.20s\n",
      "Epoch 69/200, Train Loss: 107.0132, Val Loss: 701.4722, Train MSE: 108.6391, Val MSE: 106.9686, Train RMSE: 10.4230, Val RMSE: 10.3426, Time: 0.22s\n",
      "Epoch 70/200, Train Loss: 109.2811, Val Loss: 700.6669, Train MSE: 108.3071, Val MSE: 105.7227, Train RMSE: 10.4071, Val RMSE: 10.2822, Time: 0.21s\n",
      "Epoch 71/200, Train Loss: 104.0629, Val Loss: 675.6168, Train MSE: 105.4525, Val MSE: 104.4837, Train RMSE: 10.2690, Val RMSE: 10.2217, Time: 0.23s\n",
      "Epoch 72/200, Train Loss: 106.3834, Val Loss: 674.1496, Train MSE: 105.4424, Val MSE: 103.3707, Train RMSE: 10.2685, Val RMSE: 10.1671, Time: 0.26s\n",
      "Epoch 73/200, Train Loss: 102.2745, Val Loss: 657.8486, Train MSE: 103.4591, Val MSE: 102.2474, Train RMSE: 10.1715, Val RMSE: 10.1117, Time: 0.21s\n",
      "Epoch 74/200, Train Loss: 101.2612, Val Loss: 654.5333, Train MSE: 101.8612, Val MSE: 101.1362, Train RMSE: 10.0926, Val RMSE: 10.0566, Time: 0.25s\n",
      "Epoch 75/200, Train Loss: 101.9631, Val Loss: 640.5431, Train MSE: 101.7486, Val MSE: 100.2003, Train RMSE: 10.0870, Val RMSE: 10.0100, Time: 0.29s\n",
      "Epoch 76/200, Train Loss: 100.8807, Val Loss: 653.0720, Train MSE: 101.4309, Val MSE: 99.2773, Train RMSE: 10.0713, Val RMSE: 9.9638, Time: 0.22s\n",
      "Epoch 77/200, Train Loss: 99.5022, Val Loss: 654.5798, Train MSE: 100.3129, Val MSE: 98.3990, Train RMSE: 10.0156, Val RMSE: 9.9196, Time: 0.22s\n",
      "Epoch 78/200, Train Loss: 101.2543, Val Loss: 654.3821, Train MSE: 101.2048, Val MSE: 97.5875, Train RMSE: 10.0601, Val RMSE: 9.8786, Time: 0.22s\n",
      "Epoch 79/200, Train Loss: 99.6728, Val Loss: 645.0872, Train MSE: 99.1137, Val MSE: 96.7747, Train RMSE: 9.9556, Val RMSE: 9.8374, Time: 0.22s\n",
      "Epoch 80/200, Train Loss: 97.0937, Val Loss: 615.8213, Train MSE: 97.2182, Val MSE: 96.0010, Train RMSE: 9.8599, Val RMSE: 9.7980, Time: 0.22s\n",
      "Epoch 81/200, Train Loss: 98.4000, Val Loss: 613.7631, Train MSE: 97.6831, Val MSE: 95.3018, Train RMSE: 9.8835, Val RMSE: 9.7623, Time: 0.23s\n",
      "Epoch 82/200, Train Loss: 97.0391, Val Loss: 611.4662, Train MSE: 96.5988, Val MSE: 94.6046, Train RMSE: 9.8285, Val RMSE: 9.7265, Time: 0.22s\n",
      "Epoch 83/200, Train Loss: 99.6193, Val Loss: 608.7312, Train MSE: 97.8523, Val MSE: 93.9613, Train RMSE: 9.8920, Val RMSE: 9.6934, Time: 0.24s\n",
      "Epoch 84/200, Train Loss: 95.6428, Val Loss: 618.0048, Train MSE: 96.7248, Val MSE: 93.3363, Train RMSE: 9.8349, Val RMSE: 9.6611, Time: 0.23s\n",
      "Epoch 85/200, Train Loss: 93.4693, Val Loss: 605.6004, Train MSE: 93.4865, Val MSE: 92.7789, Train RMSE: 9.6688, Val RMSE: 9.6322, Time: 0.23s\n",
      "Epoch 86/200, Train Loss: 94.9030, Val Loss: 607.5346, Train MSE: 95.0734, Val MSE: 92.2125, Train RMSE: 9.7506, Val RMSE: 9.6027, Time: 0.26s\n",
      "Epoch 87/200, Train Loss: 93.8016, Val Loss: 589.5418, Train MSE: 95.1917, Val MSE: 91.7125, Train RMSE: 9.7566, Val RMSE: 9.5767, Time: 0.22s\n",
      "Epoch 88/200, Train Loss: 93.6494, Val Loss: 582.0620, Train MSE: 93.6689, Val MSE: 91.2819, Train RMSE: 9.6783, Val RMSE: 9.5542, Time: 0.22s\n",
      "Epoch 89/200, Train Loss: 95.1210, Val Loss: 586.7649, Train MSE: 95.0378, Val MSE: 90.8040, Train RMSE: 9.7487, Val RMSE: 9.5291, Time: 0.22s\n",
      "Epoch 90/200, Train Loss: 93.4052, Val Loss: 575.3142, Train MSE: 93.4064, Val MSE: 90.3637, Train RMSE: 9.6647, Val RMSE: 9.5060, Time: 0.22s\n",
      "Epoch 91/200, Train Loss: 94.2020, Val Loss: 580.2134, Train MSE: 91.4331, Val MSE: 89.9977, Train RMSE: 9.5621, Val RMSE: 9.4867, Time: 0.25s\n",
      "Epoch 92/200, Train Loss: 89.2824, Val Loss: 571.8419, Train MSE: 90.2687, Val MSE: 89.5764, Train RMSE: 9.5010, Val RMSE: 9.4645, Time: 0.24s\n",
      "Epoch 93/200, Train Loss: 91.0261, Val Loss: 577.4356, Train MSE: 90.2334, Val MSE: 89.2025, Train RMSE: 9.4991, Val RMSE: 9.4447, Time: 0.29s\n",
      "Epoch 94/200, Train Loss: 90.1300, Val Loss: 586.0710, Train MSE: 91.6695, Val MSE: 88.8678, Train RMSE: 9.5744, Val RMSE: 9.4270, Time: 0.29s\n",
      "Epoch 95/200, Train Loss: 91.3252, Val Loss: 578.2752, Train MSE: 89.6156, Val MSE: 88.5844, Train RMSE: 9.4665, Val RMSE: 9.4119, Time: 0.22s\n",
      "Epoch 96/200, Train Loss: 91.4150, Val Loss: 584.2770, Train MSE: 91.3157, Val MSE: 88.2697, Train RMSE: 9.5559, Val RMSE: 9.3952, Time: 0.24s\n",
      "Epoch 97/200, Train Loss: 89.6240, Val Loss: 586.1061, Train MSE: 88.6386, Val MSE: 87.9756, Train RMSE: 9.4148, Val RMSE: 9.3795, Time: 0.23s\n",
      "Epoch 98/200, Train Loss: 91.3159, Val Loss: 568.1797, Train MSE: 90.1745, Val MSE: 87.7056, Train RMSE: 9.4960, Val RMSE: 9.3651, Time: 0.24s\n",
      "Epoch 99/200, Train Loss: 87.8916, Val Loss: 572.9867, Train MSE: 88.8065, Val MSE: 87.4604, Train RMSE: 9.4237, Val RMSE: 9.3520, Time: 0.24s\n",
      "Epoch 100/200, Train Loss: 89.2207, Val Loss: 579.5520, Train MSE: 89.7698, Val MSE: 87.2640, Train RMSE: 9.4747, Val RMSE: 9.3415, Time: 0.27s\n",
      "Epoch 101/200, Train Loss: 88.4471, Val Loss: 565.3098, Train MSE: 89.1741, Val MSE: 87.0449, Train RMSE: 9.4432, Val RMSE: 9.3298, Time: 0.25s\n",
      "Epoch 102/200, Train Loss: 89.7337, Val Loss: 561.6164, Train MSE: 89.0858, Val MSE: 86.8403, Train RMSE: 9.4385, Val RMSE: 9.3188, Time: 0.22s\n",
      "Epoch 103/200, Train Loss: 87.0924, Val Loss: 563.7279, Train MSE: 86.9676, Val MSE: 86.6532, Train RMSE: 9.3256, Val RMSE: 9.3088, Time: 0.22s\n",
      "Epoch 104/200, Train Loss: 89.0431, Val Loss: 554.7547, Train MSE: 87.7993, Val MSE: 86.4860, Train RMSE: 9.3701, Val RMSE: 9.2998, Time: 0.26s\n",
      "Epoch 105/200, Train Loss: 89.3063, Val Loss: 552.2691, Train MSE: 87.4245, Val MSE: 86.3157, Train RMSE: 9.3501, Val RMSE: 9.2906, Time: 0.24s\n",
      "Epoch 106/200, Train Loss: 87.3970, Val Loss: 550.1378, Train MSE: 87.6687, Val MSE: 86.1564, Train RMSE: 9.3632, Val RMSE: 9.2820, Time: 0.22s\n",
      "Epoch 107/200, Train Loss: 85.4440, Val Loss: 558.4807, Train MSE: 86.2145, Val MSE: 86.0199, Train RMSE: 9.2852, Val RMSE: 9.2747, Time: 0.23s\n",
      "Epoch 108/200, Train Loss: 87.6485, Val Loss: 554.9572, Train MSE: 87.5227, Val MSE: 85.8733, Train RMSE: 9.3554, Val RMSE: 9.2668, Time: 0.22s\n",
      "Epoch 109/200, Train Loss: 88.7153, Val Loss: 557.1368, Train MSE: 89.7562, Val MSE: 85.7469, Train RMSE: 9.4740, Val RMSE: 9.2600, Time: 0.23s\n",
      "Epoch 110/200, Train Loss: 84.3594, Val Loss: 557.3690, Train MSE: 85.7306, Val MSE: 85.6165, Train RMSE: 9.2591, Val RMSE: 9.2529, Time: 0.22s\n",
      "Epoch 111/200, Train Loss: 88.4559, Val Loss: 543.3323, Train MSE: 87.1307, Val MSE: 85.5113, Train RMSE: 9.3344, Val RMSE: 9.2472, Time: 0.24s\n",
      "Epoch 112/200, Train Loss: 87.2542, Val Loss: 568.4973, Train MSE: 87.2001, Val MSE: 85.3990, Train RMSE: 9.3381, Val RMSE: 9.2412, Time: 0.23s\n",
      "Epoch 113/200, Train Loss: 85.4627, Val Loss: 559.7545, Train MSE: 86.5245, Val MSE: 85.2982, Train RMSE: 9.3019, Val RMSE: 9.2357, Time: 0.24s\n",
      "Epoch 114/200, Train Loss: 89.5789, Val Loss: 568.3846, Train MSE: 89.1899, Val MSE: 85.2183, Train RMSE: 9.4440, Val RMSE: 9.2314, Time: 0.23s\n",
      "Epoch 115/200, Train Loss: 88.0904, Val Loss: 547.9026, Train MSE: 88.5615, Val MSE: 85.1488, Train RMSE: 9.4107, Val RMSE: 9.2276, Time: 0.21s\n",
      "Epoch 116/200, Train Loss: 84.9689, Val Loss: 546.0722, Train MSE: 85.7955, Val MSE: 85.0857, Train RMSE: 9.2626, Val RMSE: 9.2242, Time: 0.25s\n",
      "Epoch 117/200, Train Loss: 84.9834, Val Loss: 543.8679, Train MSE: 85.1133, Val MSE: 85.0197, Train RMSE: 9.2257, Val RMSE: 9.2206, Time: 0.22s\n",
      "Epoch 118/200, Train Loss: 85.3894, Val Loss: 552.8136, Train MSE: 85.8338, Val MSE: 84.9402, Train RMSE: 9.2647, Val RMSE: 9.2163, Time: 0.26s\n",
      "Epoch 119/200, Train Loss: 84.3998, Val Loss: 558.7451, Train MSE: 85.2538, Val MSE: 84.8841, Train RMSE: 9.2333, Val RMSE: 9.2133, Time: 0.27s\n",
      "Epoch 120/200, Train Loss: 87.3468, Val Loss: 563.3928, Train MSE: 87.4269, Val MSE: 84.8218, Train RMSE: 9.3502, Val RMSE: 9.2099, Time: 0.29s\n",
      "Epoch 121/200, Train Loss: 85.5954, Val Loss: 542.3387, Train MSE: 86.2994, Val MSE: 84.7614, Train RMSE: 9.2897, Val RMSE: 9.2066, Time: 0.28s\n",
      "Epoch 122/200, Train Loss: 89.1052, Val Loss: 540.9987, Train MSE: 88.6650, Val MSE: 84.7166, Train RMSE: 9.4162, Val RMSE: 9.2042, Time: 0.26s\n",
      "Epoch 123/200, Train Loss: 84.9220, Val Loss: 543.6992, Train MSE: 85.6899, Val MSE: 84.6795, Train RMSE: 9.2569, Val RMSE: 9.2021, Time: 0.22s\n",
      "Epoch 124/200, Train Loss: 89.8189, Val Loss: 544.9647, Train MSE: 87.4543, Val MSE: 84.6403, Train RMSE: 9.3517, Val RMSE: 9.2000, Time: 0.23s\n",
      "Epoch 125/200, Train Loss: 87.7178, Val Loss: 565.6261, Train MSE: 88.7917, Val MSE: 84.5962, Train RMSE: 9.4229, Val RMSE: 9.1976, Time: 0.26s\n",
      "Epoch 126/200, Train Loss: 86.8577, Val Loss: 554.1817, Train MSE: 87.1777, Val MSE: 84.5613, Train RMSE: 9.3369, Val RMSE: 9.1957, Time: 0.24s\n",
      "Epoch 127/200, Train Loss: 87.2427, Val Loss: 555.5498, Train MSE: 87.0047, Val MSE: 84.5397, Train RMSE: 9.3276, Val RMSE: 9.1945, Time: 0.23s\n",
      "Epoch 128/200, Train Loss: 85.6781, Val Loss: 565.0015, Train MSE: 86.0515, Val MSE: 84.5054, Train RMSE: 9.2764, Val RMSE: 9.1927, Time: 0.24s\n",
      "Epoch 129/200, Train Loss: 85.3387, Val Loss: 542.2613, Train MSE: 85.3646, Val MSE: 84.4728, Train RMSE: 9.2393, Val RMSE: 9.1909, Time: 0.26s\n",
      "Epoch 130/200, Train Loss: 88.2979, Val Loss: 551.2464, Train MSE: 86.9228, Val MSE: 84.4416, Train RMSE: 9.3232, Val RMSE: 9.1892, Time: 0.25s\n",
      "Epoch 131/200, Train Loss: 83.6644, Val Loss: 545.8521, Train MSE: 84.2647, Val MSE: 84.4211, Train RMSE: 9.1796, Val RMSE: 9.1881, Time: 0.25s\n",
      "Epoch 132/200, Train Loss: 89.2310, Val Loss: 556.4819, Train MSE: 89.0305, Val MSE: 84.3904, Train RMSE: 9.4356, Val RMSE: 9.1864, Time: 0.24s\n",
      "Epoch 133/200, Train Loss: 86.9264, Val Loss: 551.7334, Train MSE: 87.0535, Val MSE: 84.3655, Train RMSE: 9.3302, Val RMSE: 9.1851, Time: 0.24s\n",
      "Epoch 134/200, Train Loss: 87.6867, Val Loss: 541.9708, Train MSE: 86.3643, Val MSE: 84.3648, Train RMSE: 9.2932, Val RMSE: 9.1850, Time: 0.26s\n",
      "Epoch 135/200, Train Loss: 87.6638, Val Loss: 539.0467, Train MSE: 85.7952, Val MSE: 84.3637, Train RMSE: 9.2626, Val RMSE: 9.1850, Time: 0.24s\n",
      "Epoch 136/200, Train Loss: 84.5697, Val Loss: 543.6497, Train MSE: 86.8089, Val MSE: 84.3608, Train RMSE: 9.3171, Val RMSE: 9.1848, Time: 0.28s\n",
      "Epoch 137/200, Train Loss: 87.1007, Val Loss: 543.3379, Train MSE: 86.6430, Val MSE: 84.3594, Train RMSE: 9.3082, Val RMSE: 9.1847, Time: 0.28s\n",
      "Epoch 138/200, Train Loss: 88.7619, Val Loss: 542.6845, Train MSE: 87.6156, Val MSE: 84.3563, Train RMSE: 9.3603, Val RMSE: 9.1846, Time: 0.26s\n",
      "Epoch 139/200, Train Loss: 86.1061, Val Loss: 548.1901, Train MSE: 85.8479, Val MSE: 84.3546, Train RMSE: 9.2654, Val RMSE: 9.1845, Time: 0.22s\n",
      "Epoch 140/200, Train Loss: 88.3143, Val Loss: 563.9267, Train MSE: 89.2659, Val MSE: 84.3532, Train RMSE: 9.4481, Val RMSE: 9.1844, Time: 0.23s\n",
      "Epoch 141/200, Train Loss: 85.9326, Val Loss: 555.0547, Train MSE: 86.1177, Val MSE: 84.3523, Train RMSE: 9.2800, Val RMSE: 9.1844, Time: 0.22s\n",
      "Epoch 142/200, Train Loss: 84.5130, Val Loss: 538.3259, Train MSE: 84.9928, Val MSE: 84.3505, Train RMSE: 9.2192, Val RMSE: 9.1843, Time: 0.23s\n",
      "Epoch 143/200, Train Loss: 86.4074, Val Loss: 565.3364, Train MSE: 85.9383, Val MSE: 84.3483, Train RMSE: 9.2703, Val RMSE: 9.1841, Time: 0.24s\n",
      "Epoch 144/200, Train Loss: 85.7743, Val Loss: 542.7502, Train MSE: 85.2306, Val MSE: 84.3473, Train RMSE: 9.2320, Val RMSE: 9.1841, Time: 0.24s\n",
      "Epoch 145/200, Train Loss: 89.6398, Val Loss: 540.2219, Train MSE: 88.8333, Val MSE: 84.3439, Train RMSE: 9.4251, Val RMSE: 9.1839, Time: 0.28s\n",
      "Epoch 146/200, Train Loss: 85.6510, Val Loss: 541.5675, Train MSE: 85.9995, Val MSE: 84.3418, Train RMSE: 9.2736, Val RMSE: 9.1838, Time: 0.22s\n",
      "Epoch 147/200, Train Loss: 88.0060, Val Loss: 551.5076, Train MSE: 87.8929, Val MSE: 84.3401, Train RMSE: 9.3751, Val RMSE: 9.1837, Time: 0.24s\n",
      "Epoch 148/200, Train Loss: 84.7239, Val Loss: 551.7521, Train MSE: 86.0711, Val MSE: 84.3380, Train RMSE: 9.2775, Val RMSE: 9.1836, Time: 0.22s\n",
      "Epoch 149/200, Train Loss: 87.5046, Val Loss: 542.1972, Train MSE: 86.4764, Val MSE: 84.3369, Train RMSE: 9.2993, Val RMSE: 9.1835, Time: 0.27s\n",
      "Epoch 150/200, Train Loss: 87.4573, Val Loss: 540.3278, Train MSE: 87.2956, Val MSE: 84.3348, Train RMSE: 9.3432, Val RMSE: 9.1834, Time: 0.25s\n",
      "Epoch 151/200, Train Loss: 86.5519, Val Loss: 539.9911, Train MSE: 86.9806, Val MSE: 84.3333, Train RMSE: 9.3263, Val RMSE: 9.1833, Time: 0.23s\n",
      "Epoch 152/200, Train Loss: 85.5900, Val Loss: 545.5215, Train MSE: 84.6108, Val MSE: 84.3322, Train RMSE: 9.1984, Val RMSE: 9.1833, Time: 0.23s\n",
      "Epoch 153/200, Train Loss: 86.3781, Val Loss: 559.7453, Train MSE: 85.9736, Val MSE: 84.3301, Train RMSE: 9.2722, Val RMSE: 9.1831, Time: 0.22s\n",
      "Epoch 154/200, Train Loss: 86.2762, Val Loss: 536.6196, Train MSE: 84.6596, Val MSE: 84.3299, Train RMSE: 9.2011, Val RMSE: 9.1831, Time: 0.24s\n",
      "Epoch 155/200, Train Loss: 88.6481, Val Loss: 550.9391, Train MSE: 86.6283, Val MSE: 84.3298, Train RMSE: 9.3074, Val RMSE: 9.1831, Time: 0.24s\n",
      "Epoch 156/200, Train Loss: 87.4738, Val Loss: 541.2516, Train MSE: 87.0190, Val MSE: 84.3296, Train RMSE: 9.3284, Val RMSE: 9.1831, Time: 0.25s\n",
      "Epoch 157/200, Train Loss: 84.5512, Val Loss: 538.9189, Train MSE: 85.1056, Val MSE: 84.3294, Train RMSE: 9.2253, Val RMSE: 9.1831, Time: 0.23s\n",
      "Epoch 158/200, Train Loss: 88.0282, Val Loss: 561.5296, Train MSE: 87.6781, Val MSE: 84.3292, Train RMSE: 9.3637, Val RMSE: 9.1831, Time: 0.22s\n",
      "Epoch 159/200, Train Loss: 84.4386, Val Loss: 543.6895, Train MSE: 85.5758, Val MSE: 84.3291, Train RMSE: 9.2507, Val RMSE: 9.1831, Time: 0.23s\n",
      "Epoch 160/200, Train Loss: 89.1066, Val Loss: 548.9629, Train MSE: 87.6630, Val MSE: 84.3291, Train RMSE: 9.3629, Val RMSE: 9.1831, Time: 0.23s\n",
      "Epoch 161/200, Train Loss: 88.2852, Val Loss: 557.6844, Train MSE: 87.2666, Val MSE: 84.3289, Train RMSE: 9.3417, Val RMSE: 9.1831, Time: 0.25s\n",
      "Epoch 162/200, Train Loss: 84.0872, Val Loss: 555.5564, Train MSE: 84.0805, Val MSE: 84.3287, Train RMSE: 9.1695, Val RMSE: 9.1831, Time: 0.23s\n",
      "Epoch 163/200, Train Loss: 85.8806, Val Loss: 547.0594, Train MSE: 87.0825, Val MSE: 84.3285, Train RMSE: 9.3318, Val RMSE: 9.1831, Time: 0.25s\n",
      "Epoch 164/200, Train Loss: 86.4094, Val Loss: 537.9765, Train MSE: 84.9430, Val MSE: 84.3283, Train RMSE: 9.2165, Val RMSE: 9.1830, Time: 0.25s\n",
      "Epoch 165/200, Train Loss: 84.5656, Val Loss: 546.0298, Train MSE: 84.6540, Val MSE: 84.3281, Train RMSE: 9.2008, Val RMSE: 9.1830, Time: 0.25s\n",
      "Epoch 166/200, Train Loss: 87.1371, Val Loss: 553.2168, Train MSE: 86.1306, Val MSE: 84.3280, Train RMSE: 9.2807, Val RMSE: 9.1830, Time: 0.24s\n",
      "Epoch 167/200, Train Loss: 86.9384, Val Loss: 564.0187, Train MSE: 87.9348, Val MSE: 84.3280, Train RMSE: 9.3774, Val RMSE: 9.1830, Time: 0.24s\n",
      "Epoch 168/200, Train Loss: 84.8696, Val Loss: 545.3231, Train MSE: 85.3341, Val MSE: 84.3280, Train RMSE: 9.2376, Val RMSE: 9.1830, Time: 0.25s\n",
      "Epoch 169/200, Train Loss: 85.8122, Val Loss: 551.3277, Train MSE: 86.8866, Val MSE: 84.3280, Train RMSE: 9.3213, Val RMSE: 9.1830, Time: 0.23s\n",
      "Epoch 170/200, Train Loss: 86.5707, Val Loss: 542.6290, Train MSE: 87.2024, Val MSE: 84.3280, Train RMSE: 9.3382, Val RMSE: 9.1830, Time: 0.22s\n",
      "Epoch 171/200, Train Loss: 86.5750, Val Loss: 545.8858, Train MSE: 86.9825, Val MSE: 84.3280, Train RMSE: 9.3264, Val RMSE: 9.1830, Time: 0.26s\n",
      "Epoch 172/200, Train Loss: 83.6743, Val Loss: 559.5833, Train MSE: 85.4517, Val MSE: 84.3279, Train RMSE: 9.2440, Val RMSE: 9.1830, Time: 0.23s\n",
      "Epoch 173/200, Train Loss: 86.3236, Val Loss: 538.8542, Train MSE: 86.2562, Val MSE: 84.3279, Train RMSE: 9.2874, Val RMSE: 9.1830, Time: 0.25s\n",
      "Epoch 174/200, Train Loss: 88.9074, Val Loss: 555.4243, Train MSE: 87.5131, Val MSE: 84.3279, Train RMSE: 9.3548, Val RMSE: 9.1830, Time: 0.23s\n",
      "Early stopping\n",
      "Model 1 Metrics: {'train_loss': 571.1113985501803, 'train_mse': 572.04083, 'train_mae': 22.088606, 'train_rmse': 23.917376, 'train_r2': -5.79913854598999, 'val_loss': 497.6116027832031, 'val_mse': 507.00757, 'val_mae': 21.129644, 'val_rmse': 22.516829, 'val_r2': -7.373939514160156, 'test_loss': 690.2221069335938, 'test_mse': 680.95044, 'test_mae': 23.97474, 'test_rmse': 26.095028, 'test_r2': -5.41424036026001, 'total_time': 5.906403541564941, 'average_epoch_time': 0.07666090246918913}\n",
      "Model 2 Metrics: {'train_loss': 12.85397140796368, 'train_mse': 13.039645, 'train_mae': 2.303319, 'train_rmse': 3.611045, 'train_r2': 0.8450139760971069, 'val_loss': 13.313889026641846, 'val_mse': 13.757009, 'val_mae': 2.8287818, 'val_rmse': 3.7090442, 'val_r2': 0.7727837562561035, 'test_loss': 23.359837293624878, 'test_mse': 27.314873, 'test_mae': 3.0114183, 'test_rmse': 5.226363, 'test_r2': 0.7427064180374146, 'total_time': 17.292457342147827, 'average_epoch_time': 0.09540285326499306}\n",
      "Model 3 Metrics: {'train_loss': 84.47095196063702, 'train_mse': 84.3279, 'train_mae': 6.5056224, 'train_rmse': 9.1830225, 'train_r2': -0.002300858497619629, 'val_loss': 54.99611473083496, 'val_mse': 60.815235, 'val_mae': 5.7745223, 'val_rmse': 7.7984123, 'val_r2': -0.004448652267456055, 'test_loss': 112.96853637695312, 'test_mse': 111.5731, 'test_mae': 7.8783946, 'test_rmse': 10.562817, 'test_r2': -0.05096733570098877, 'total_time': 41.76979088783264, 'average_epoch_time': 0.23941929587002458}\n"
     ]
    }
   ],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, option=\"maximum\", random_seed=SEED)\n",
    "name = f\"TINTO_blur_maximum\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, random_seed=SEED)\n",
    "name = f\"TINTO\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 2: IGTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the dataframe\n",
    "num_columns = df.shape[1]\n",
    "\n",
    "# Calculate number of columns - 1\n",
    "columns_minus_one = num_columns - 1\n",
    "\n",
    "# Calculate the square root for image size\n",
    "import math\n",
    "image_size = math.ceil(math.sqrt(columns_minus_one))\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], random_seed=SEED)\n",
    "name = f\"IGTD_{image_size}x{image_size}\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], zoom=2, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size*2}x{image_size*2}\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], zoom=4, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size*4}x{image_size*4}\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], fea_dist_method='Euclidean', image_dist_method='Euclidean', random_seed=SEED)\n",
    "name = f\"IGTD_{image_size}x{image_size}_fEuclidean_iEuclidean\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 3: REFINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = REFINED(problem= problem_type, random_seed=SEED)\n",
    "name = f\"REFINED\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = REFINED(problem= problem_type, zoom=2, random_seed=SEED)\n",
    "name = f\"REFINED_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = REFINED(problem= problem_type, zoom=4, random_seed=SEED)\n",
    "name = f\"REFINED_zoom4\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 4: BAR GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = BarGraph(problem= problem_type)\n",
    "name = f\"BarGraph\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = BarGraph(problem= problem_type, zoom=2)\n",
    "name = f\"BarGraph_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = BarGraph(problem= problem_type, zoom=4)\n",
    "name = f\"BarGraph_zoom4\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 5: DISTANCE MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = DistanceMatrix(problem= problem_type)\n",
    "name = f\"DistanceMatrix\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = DistanceMatrix(problem= problem_type, zoom=2)\n",
    "name = f\"DistanceMatrix_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = DistanceMatrix(problem= problem_type, zoom=4)\n",
    "name = f\"DistanceMatrix_zoom4\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 6: COMBINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = Combination(problem= problem_type)\n",
    "name = f\"Combination\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = Combination(problem= problem_type, zoom=2)\n",
    "name = f\"Combination_zoom2\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = Combination(problem= problem_type, zoom=4)\n",
    "name = f\"Combination_zoom4\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 7: SUPERTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = SuperTML(problem= problem_type, random_seed=SEED)\n",
    "name = f\"SuperTML-EF\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = SuperTML(problem= problem_type, feature_importance=True, font_size=30, random_seed=SEED)\n",
    "name = f\"SuperTML-VF_FS30\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, X_test_num, X_train_img, X_val_img, X_test_img, y_train, y_val, y_test, imgs_shape, attributes = load_and_preprocess_data(images_folder, image_model, problem_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = try_create_model(Model1, imgs_shape)\n",
    "model2 = try_create_model(Model2, imgs_shape)\n",
    "model3 = try_create_model(Model3, imgs_shape)\n",
    "\n",
    "# Example usage with two models\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model1\")\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model2\")\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"{name}_Model3\")\n",
    "\n",
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TINTO-P",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
